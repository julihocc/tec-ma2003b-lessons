{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b821a6ff",
   "metadata": {},
   "source": [
    "# Factor Analysis: Educational Assessment Data\n",
    "\n",
    "This analysis demonstrates Factor Analysis (FA) to identify latent constructs\n",
    "underlying student assessment performance. We'll explore how FA differs from PCA\n",
    "by focusing on shared variance and latent factor interpretation.\n",
    "\n",
    "**Learning objectives:**\n",
    "- Apply Factor Analysis to discover latent psychological constructs\n",
    "- Understand communalities and uniquenesses in measurement models\n",
    "- Use factor rotation to achieve simple structure\n",
    "- Compare Factor Analysis with Principal Component Analysis\n",
    "- Interpret factor loadings for construct validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4a62fc",
   "metadata": {},
   "source": [
    "## Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f1d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Simple logger\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3727e02f",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4258e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = Path(__file__).resolve().parent\n",
    "data_path = script_dir / \"educational.csv\"\n",
    "\n",
    "if not data_path.exists():\n",
    "    logger.error(f\"Data file not found: {data_path}\")\n",
    "    logger.info(\"Run 'fetch_educational.py' to generate the required data file\")\n",
    "    sys.exit(1)\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "logger.info(\n",
    "    f\"Loaded dataset: {len(df)} students, {len(df.columns) - 1} assessment variables\"\n",
    ")\n",
    "\n",
    "# Extract assessment variables (exclude Student ID)\n",
    "X = df.iloc[:, 1:]\n",
    "variable_names = list(X.columns)\n",
    "\n",
    "logger.info(f\"Assessment variables: {variable_names}\")\n",
    "logger.info(f\"Data shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5e8a0d",
   "metadata": {},
   "source": [
    "## Data Standardization\n",
    "\n",
    "Factor Analysis requires standardized data to ensure variables contribute equally\n",
    "to the factor solution, regardless of their original measurement scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fa97ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "logger.info(\"Data standardized: mean ≈ 0, std ≈ 1 for all variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cdf063",
   "metadata": {},
   "source": [
    "## Factor Analysis Assumptions Testing\n",
    "\n",
    "Before proceeding with Factor Analysis, we must verify that our data meets\n",
    "key statistical assumptions for meaningful factor extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8010a491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test statistical assumptions\n",
    "chi_square_value, p_value = calculate_bartlett_sphericity(X_standardized)\n",
    "kmo_all, kmo_model = calculate_kmo(X_standardized)\n",
    "\n",
    "logger.info(\"Factor Analysis Assumptions Testing:\")\n",
    "logger.info(\"\\nBartlett's Test of Sphericity:\")\n",
    "logger.info(f\"  Chi-square statistic: {chi_square_value:.3f}\")\n",
    "logger.info(f\"  p-value: {p_value:.6f}\")\n",
    "if p_value < 0.05:\n",
    "    logger.info(\"  ✓ Significant - variables are sufficiently correlated for FA\")\n",
    "else:\n",
    "    logger.info(\"  ✗ Not significant - FA may not be appropriate\")\n",
    "\n",
    "logger.info(\"\\nKaiser-Meyer-Olkin (KMO) Test:\")\n",
    "logger.info(f\"  Overall Measure of Sampling Adequacy: {kmo_model:.3f}\")\n",
    "if kmo_model > 0.9:\n",
    "    adequacy = \"Excellent\"\n",
    "elif kmo_model > 0.8:\n",
    "    adequacy = \"Good\"\n",
    "elif kmo_model > 0.6:\n",
    "    adequacy = \"Acceptable\"\n",
    "else:\n",
    "    adequacy = \"Unacceptable\"\n",
    "logger.info(f\"  Interpretation: {adequacy} sampling adequacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31c75c4",
   "metadata": {},
   "source": [
    "### Individual Variable Adequacy\n",
    "\n",
    "Each variable's individual KMO value indicates how well it can be predicted\n",
    "from the other variables in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92832ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"\\nIndividual Variable Sampling Adequacy:\")\n",
    "for i, var_name in enumerate(variable_names):\n",
    "    msa_value = kmo_all[i]\n",
    "    logger.info(f\"  {var_name}: {msa_value:.3f}\")\n",
    "\n",
    "# Flag any problematic variables\n",
    "low_msa_vars = [\n",
    "    var_name for i, var_name in enumerate(variable_names) if kmo_all[i] < 0.6\n",
    "]\n",
    "if low_msa_vars:\n",
    "    logger.warning(f\"Variables with low MSA (<0.6): {low_msa_vars}\")\n",
    "else:\n",
    "    logger.info(\"All variables show adequate sampling adequacy (MSA ≥ 0.6)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e743c2",
   "metadata": {},
   "source": [
    "## Factor Extraction with Principal Axis Factoring\n",
    "\n",
    "We'll extract factors using Principal Axis Factoring (PAF), which:\n",
    "- Focuses on shared variance among variables (common factors)\n",
    "- Estimates communalities iteratively\n",
    "- Distinguishes between common and unique variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4b2261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine number of factors to extract\n",
    "n_factors = 2  # Based on theoretical expectation of cognitive + social factors\n",
    "\n",
    "fa_unrotated = FactorAnalyzer(n_factors=n_factors, rotation=None, method=\"principal\")\n",
    "fa_unrotated.fit(X_standardized)\n",
    "\n",
    "# Verify successful extraction\n",
    "if fa_unrotated.loadings_ is None:\n",
    "    logger.error(\"Factor extraction failed - no loadings produced\")\n",
    "    sys.exit(1)\n",
    "\n",
    "logger.info(f\"Factor Analysis Results ({n_factors} factors extracted):\")\n",
    "eigenvalues_fa = fa_unrotated.get_eigenvalues()[0]\n",
    "logger.info(f\"Eigenvalues: {np.round(eigenvalues_fa[:n_factors], 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb566a6",
   "metadata": {},
   "source": [
    "### Communalities and Variance Decomposition\n",
    "\n",
    "Each variable's variance is decomposed into:\n",
    "- **Communality (h²)**: Variance explained by common factors\n",
    "- **Uniqueness (u²)**: Variance unique to the variable (including error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2691609",
   "metadata": {},
   "outputs": [],
   "source": [
    "communalities = fa_unrotated.get_communalities()\n",
    "uniquenesses = 1 - communalities\n",
    "\n",
    "logger.info(\"\\nVariance Decomposition (Communalities and Uniquenesses):\")\n",
    "for i, var_name in enumerate(variable_names):\n",
    "    h2 = communalities[i]\n",
    "    u2 = uniquenesses[i]\n",
    "    logger.info(f\"  {var_name}: h² = {h2:.3f}, u² = {u2:.3f}\")\n",
    "\n",
    "# Analyze overall variance structure\n",
    "factor_variance = np.sum(communalities)\n",
    "total_variance = len(variable_names)  # For standardized data\n",
    "variance_explained = factor_variance / total_variance\n",
    "\n",
    "logger.info(\"\\nOverall Variance Analysis:\")\n",
    "logger.info(f\"Total standardized variance: {total_variance:.1f}\")\n",
    "logger.info(f\"Common variance (Σh²): {factor_variance:.3f}\")\n",
    "logger.info(f\"Proportion of variance explained by factors: {variance_explained:.1%}\")\n",
    "logger.info(f\"Average communality: {np.mean(communalities):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400fad36",
   "metadata": {},
   "source": [
    "**Interpreting Communalities:**\n",
    "\n",
    "- **High communality (h² > 0.6)**: Variable strongly related to common factors\n",
    "- **Moderate communality (0.3 < h² < 0.6)**: Moderate factor relationship\n",
    "- **Low communality (h² < 0.3)**: Mostly unique variance, weak factor loading\n",
    "\n",
    "Variables with very low communalities may be candidates for removal\n",
    "or may represent additional factors not captured in the current solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442ccb1c",
   "metadata": {},
   "source": [
    "## Factor Rotation for Simple Structure\n",
    "\n",
    "Factor rotation improves interpretability without changing the fundamental solution.\n",
    "Varimax rotation seeks \"simple structure\" where each variable loads primarily on one factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e4cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_rotated = FactorAnalyzer(n_factors=n_factors, rotation=\"varimax\", method=\"principal\")\n",
    "fa_rotated.fit(X_standardized)\n",
    "\n",
    "loadings_unrotated = fa_unrotated.loadings_\n",
    "loadings_rotated = fa_rotated.loadings_\n",
    "\n",
    "# Safety check for rotation success\n",
    "if loadings_rotated is None:\n",
    "    logger.warning(\"Varimax rotation failed, using unrotated solution\")\n",
    "    loadings_rotated = loadings_unrotated\n",
    "    fa_rotated = fa_unrotated\n",
    "\n",
    "logger.info(\"\\nFactor Loadings Comparison (Unrotated vs. Varimax Rotated):\")\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Variable\": variable_names,\n",
    "        \"Unrot_F1\": loadings_unrotated[:, 0],\n",
    "        \"Unrot_F2\": loadings_unrotated[:, 1],\n",
    "        \"Rotated_F1\": loadings_rotated[:, 0],\n",
    "        \"Rotated_F2\": loadings_rotated[:, 1],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(comparison_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e186e",
   "metadata": {},
   "source": [
    "### Factor Loading Interpretation\n",
    "\n",
    "**Rotation benefits:**\n",
    "- **Simple structure**: Variables load primarily on one factor\n",
    "- **Clearer interpretation**: Easier to identify what each factor represents\n",
    "- **Practical meaning**: Factors align better with theoretical constructs\n",
    "\n",
    "**Loading interpretation guidelines:**\n",
    "- **|loading| > 0.6**: Strong factor relationship\n",
    "- **0.3 < |loading| < 0.6**: Moderate relationship\n",
    "- **|loading| < 0.3**: Weak/negligible relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ce4e1",
   "metadata": {},
   "source": [
    "## Factor Loading Visualization\n",
    "\n",
    "Heatmaps provide visual comparison of loading patterns before and after rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Unrotated loadings heatmap\n",
    "sns.heatmap(\n",
    "    loadings_unrotated.T,\n",
    "    annot=True,\n",
    "    fmt=\".3f\",\n",
    "    xticklabels=variable_names,\n",
    "    yticklabels=[f\"Factor {i + 1}\" for i in range(n_factors)],\n",
    "    cmap=\"RdBu_r\",\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    ax=ax1,\n",
    "    cbar_kws={\"shrink\": 0.8},\n",
    ")\n",
    "ax1.set_title(\"Unrotated Factor Loadings\")\n",
    "ax1.set_xlabel(\"Variables\")\n",
    "\n",
    "# Rotated loadings heatmap\n",
    "sns.heatmap(\n",
    "    loadings_rotated.T,\n",
    "    annot=True,\n",
    "    fmt=\".3f\",\n",
    "    xticklabels=variable_names,\n",
    "    yticklabels=[f\"Factor {i + 1}\" for i in range(n_factors)],\n",
    "    cmap=\"RdBu_r\",\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    ax=ax2,\n",
    "    cbar_kws={\"shrink\": 0.8},\n",
    ")\n",
    "ax2.set_title(\"Varimax Rotated Factor Loadings\")\n",
    "ax2.set_xlabel(\"Variables\")\n",
    "\n",
    "plt.tight_layout()\n",
    "loadings_path = script_dir / \"fa_loadings.png\"\n",
    "plt.savefig(loadings_path, dpi=150, bbox_inches=\"tight\")\n",
    "logger.info(f\"Factor loadings heatmap saved: {loadings_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3281112",
   "metadata": {},
   "source": [
    "## Factor Analysis vs PCA Comparison\n",
    "\n",
    "Direct comparison reveals fundamental differences between these two approaches\n",
    "to multivariate data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fecfd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA for comparison\n",
    "pca = PCA()\n",
    "pca_scores = pca.fit_transform(X_standardized)\n",
    "pca_loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "\n",
    "# Get factor scores\n",
    "fa_scores = fa_rotated.transform(X_standardized)\n",
    "\n",
    "logger.info(\"\\nFactor Analysis vs PCA Comparison:\")\n",
    "\n",
    "# Variance explanation comparison\n",
    "pca_variance_2comp = pca.explained_variance_ratio_[:2].sum()\n",
    "logger.info(\"\\nVariance Explanation:\")\n",
    "logger.info(f\"  PCA (first 2 components): {pca_variance_2comp:.1%} of total variance\")\n",
    "logger.info(f\"  FA (2 factors): {variance_explained:.1%} of common variance\")\n",
    "logger.info(\n",
    "    \"  Key difference: PCA maximizes total variance, FA focuses on shared variance\"\n",
    ")\n",
    "\n",
    "# Loading comparison\n",
    "logger.info(\"\\nLoading Pattern Comparison:\")\n",
    "comparison_detailed = pd.DataFrame(\n",
    "    {\n",
    "        \"Variable\": variable_names,\n",
    "        \"PCA_PC1\": pca_loadings[:, 0],\n",
    "        \"PCA_PC2\": pca_loadings[:, 1],\n",
    "        \"FA_F1\": loadings_rotated[:, 0],\n",
    "        \"FA_F2\": loadings_rotated[:, 1],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(comparison_detailed.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e189b59",
   "metadata": {},
   "source": [
    "## Eigenvalue Comparison: FA vs PCA\n",
    "\n",
    "Scree plots reveal how eigenvalues differ between methods due to their\n",
    "different approaches to variance decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba841e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_eigenvalues = pca.explained_variance_\n",
    "fa_eigenvalues = fa_unrotated.get_eigenvalues()[0]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "components = np.arange(1, len(pca_eigenvalues) + 1)\n",
    "\n",
    "# PCA scree plot\n",
    "ax1.plot(\n",
    "    components,\n",
    "    pca_eigenvalues,\n",
    "    \"o-\",\n",
    "    linewidth=2,\n",
    "    color=\"steelblue\",\n",
    "    markersize=8,\n",
    "    label=\"PCA Eigenvalues\",\n",
    ")\n",
    "ax1.axhline(y=1.0, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Kaiser criterion\")\n",
    "ax1.set_xlabel(\"Component Number\")\n",
    "ax1.set_ylabel(\"Eigenvalue\")\n",
    "ax1.set_title(\"PCA Eigenvalues\")\n",
    "ax1.set_xticks(components)\n",
    "ax1.grid(True, linestyle=\":\", alpha=0.7)\n",
    "ax1.legend()\n",
    "\n",
    "# FA scree plot\n",
    "ax2.plot(\n",
    "    components,\n",
    "    fa_eigenvalues,\n",
    "    \"o-\",\n",
    "    linewidth=2,\n",
    "    color=\"darkgreen\",\n",
    "    markersize=8,\n",
    "    label=\"FA Eigenvalues\",\n",
    ")\n",
    "ax2.axhline(y=1.0, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Kaiser criterion\")\n",
    "ax2.set_xlabel(\"Factor Number\")\n",
    "ax2.set_ylabel(\"Eigenvalue\")\n",
    "ax2.set_title(\"Factor Analysis Eigenvalues\")\n",
    "ax2.set_xticks(components)\n",
    "ax2.grid(True, linestyle=\":\", alpha=0.7)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "scree_path = script_dir / \"fa_scree.png\"\n",
    "plt.savefig(scree_path, dpi=150, bbox_inches=\"tight\")\n",
    "logger.info(f\"Eigenvalue comparison saved: {scree_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c7c5a7",
   "metadata": {},
   "source": [
    "**Eigenvalue pattern differences:**\n",
    "\n",
    "- **FA eigenvalues**: Generally lower because they reflect only common variance\n",
    "- **PCA eigenvalues**: Higher because they include both common and unique variance\n",
    "- **Kaiser criterion**: May suggest different optimal numbers for FA vs PCA\n",
    "- **Theoretical focus**: FA prioritizes meaningful factors over variance maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28973046",
   "metadata": {},
   "source": [
    "## Factor Structure Interpretation and Validation\n",
    "\n",
    "Analyze the extracted factors to understand what psychological constructs\n",
    "they represent and how well they align with theoretical expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e02c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify salient loadings (commonly |loading| > 0.4)\n",
    "loading_threshold = 0.4\n",
    "\n",
    "logger.info(f\"\\nFactor Structure Analysis (threshold = {loading_threshold}):\")\n",
    "for factor_idx in range(n_factors):\n",
    "    factor_name = f\"Factor {factor_idx + 1}\"\n",
    "    salient_vars = []\n",
    "\n",
    "    for var_idx, var_name in enumerate(variable_names):\n",
    "        loading = loadings_rotated[var_idx, factor_idx]\n",
    "        if abs(loading) > loading_threshold:\n",
    "            salient_vars.append(f\"{var_name} ({loading:+.3f})\")\n",
    "\n",
    "    logger.info(\n",
    "        f\"  {factor_name}: {', '.join(salient_vars) if salient_vars else 'No salient loadings'}\"\n",
    "    )\n",
    "\n",
    "# Analyze communality patterns\n",
    "meaningful_vars = [\"MathTest\", \"VerbalTest\", \"SocialSkills\", \"Leadership\"]\n",
    "random_vars = [\"RandomVar1\", \"RandomVar2\"]\n",
    "\n",
    "if all(var in variable_names for var in meaningful_vars + random_vars):\n",
    "    meaningful_h2 = [\n",
    "        communalities[variable_names.index(var)] for var in meaningful_vars\n",
    "    ]\n",
    "    random_h2 = [communalities[variable_names.index(var)] for var in random_vars]\n",
    "\n",
    "    logger.info(\"\\nCommunality Pattern Analysis:\")\n",
    "    logger.info(\n",
    "        f\"  Meaningful variables (cognitive/social): mean h² = {np.mean(meaningful_h2):.3f}\"\n",
    "    )\n",
    "    logger.info(f\"  Random variables (noise): mean h² = {np.mean(random_h2):.3f}\")\n",
    "\n",
    "    if np.mean(meaningful_h2) > np.mean(random_h2) * 1.5:\n",
    "        logger.info(\"  ✓ Factor structure successfully distinguishes signal from noise\")\n",
    "    else:\n",
    "        logger.info(\n",
    "            \"  ⚠ Mixed signal-noise separation - consider alternative solutions\"\n",
    "        )\n",
    "else:\n",
    "    logger.info(\"\\nExpected variable names not found - skipping pattern analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38463ec0",
   "metadata": {},
   "source": [
    "## Summary and Method Selection Guidelines\n",
    "\n",
    "This Factor Analysis demonstrates key concepts for latent variable modeling:\n",
    "\n",
    "**Factor Analysis advantages demonstrated:**\n",
    "- **Theoretical grounding**: Models specific latent constructs (cognitive, social abilities)\n",
    "- **Measurement model**: Separates common variance from measurement error\n",
    "- **Simple structure**: Rotation achieves cleaner variable-factor relationships\n",
    "- **Communality estimates**: Reveals how much variance is shared vs. unique\n",
    "\n",
    "**When to choose Factor Analysis:**\n",
    "- Testing specific theories about latent psychological constructs\n",
    "- Developing or validating measurement instruments\n",
    "- Modeling common variance while acknowledging measurement error\n",
    "- Interpreting results in terms of theoretical constructs\n",
    "\n",
    "**When to choose PCA instead:**\n",
    "- Primary goal is data reduction or compression\n",
    "- Maximizing explained variance is the priority\n",
    "- No theoretical model for underlying structure\n",
    "- Computational efficiency is critical\n",
    "\n",
    "**Key methodological insights:**\n",
    "- **Assumption testing**: KMO and Bartlett's tests confirm FA appropriateness\n",
    "- **Rotation benefits**: Varimax rotation dramatically improves interpretability\n",
    "- **Communality interpretation**: Distinguishes reliable measurement from noise\n",
    "- **Construct validation**: Loading patterns should align with theoretical expectations\n",
    "\n",
    "**Next applications:**\n",
    "- Explore oblique rotation when factors may be correlated\n",
    "- Use confirmatory factor analysis to test specific theoretical models\n",
    "- Apply to real datasets where factor structure is unknown\n",
    "- Combine with other multivariate methods for comprehensive analysis"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
