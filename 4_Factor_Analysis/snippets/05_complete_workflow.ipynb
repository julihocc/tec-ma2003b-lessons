{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b99e6a1f",
   "metadata": {},
   "source": [
    "# Complete Factor Analysis Workflow - MA2003B Multivariate Statistics Course\n",
    "\n",
    "This notebook demonstrates a comprehensive end-to-end Factor Analysis pipeline, from data preparation through interpretation and comparison with PCA. This represents the complete analytical process used in multivariate statistics.\n",
    "\n",
    "## Learning Objectives:\n",
    "- Implement the full factor analysis workflow\n",
    "- Apply statistical tests for factorability\n",
    "- Make decisions about factor retention and rotation\n",
    "- Interpret factor loadings, communalities, and variance explained\n",
    "- Compare Factor Analysis results with Principal Component Analysis\n",
    "\n",
    "**Data**: Simulated 100 observations × 5 variables (replace with real data)\n",
    "\n",
    "**Workflow Steps**:\n",
    "1. Data preparation and standardization\n",
    "2. Factorability assessment (KMO, Bartlett's test)\n",
    "3. Factor extraction and retention decisions\n",
    "4. Factor rotation for interpretability\n",
    "5. Results interpretation and validation\n",
    "6. Comparison with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11d2d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f68954df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Factor Analysis Workflow\n",
      "==================================================\n",
      "Following systematic steps for multivariate analysis\n"
     ]
    }
   ],
   "source": [
    "# Complete Factor Analysis Workflow\n",
    "print(\"Complete Factor Analysis Workflow\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Following systematic steps for multivariate analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa4a207",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6426dede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Data Preparation\n",
      "-------------------------\n",
      "Dataset dimensions: 100 observations × 5 variables\n",
      "Data type: Simulated multivariate normal (replace with real data)\n"
     ]
    }
   ],
   "source": [
    "# Load your actual data here (replace the simulated data)\n",
    "# X = pd.read_csv('your_data.csv')  # Real data loading\n",
    "# For demonstration, we use simulated multivariate normal data\n",
    "np.random.seed(42)  # For reproducible results\n",
    "X = np.random.randn(100, 5)  # 100 observations, 5 variables\n",
    "\n",
    "print(\"Step 1: Data Preparation\")\n",
    "print(\"-\" * 25)\n",
    "print(f\"Dataset dimensions: {X.shape[0]} observations × {X.shape[1]} variables\")\n",
    "print(\"Data type: Simulated multivariate normal (replace with real data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf0bd5c",
   "metadata": {},
   "source": [
    "## Step 2: Data Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff8d8e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Data Standardization\n",
      "-------------------------\n",
      "Variables standardized: mean ≈ 0, std ≈ 1 for all variables\n",
      "Ensures equal contribution regardless of original scales\n"
     ]
    }
   ],
   "source": [
    "# Standardize variables to ensure equal contribution to analysis\n",
    "# This is crucial when variables have different scales/units\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Step 2: Data Standardization\")\n",
    "print(\"-\" * 25)\n",
    "print(\"Variables standardized: mean ≈ 0, std ≈ 1 for all variables\")\n",
    "print(\"Ensures equal contribution regardless of original scales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829d7dfb",
   "metadata": {},
   "source": [
    "## Step 3: Assess Suitability for Factor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc2778",
   "metadata": {},
   "outputs": [],
   "source": "# Test statistical assumptions before proceeding\nkmo_all, kmo_model = calculate_kmo(X_scaled)\nchi_square_value, p_value = calculate_bartlett_sphericity(X_scaled)\n\nprint(\"Step 3: Factorability Assessment\")\nprint(\"-\" * 25)\nprint(\"Testing whether data is suitable for factor analysis:\")\nprint()\n\nprint(\"Kaiser-Meyer-Olkin (KMO) Test:\")\nprint(\"  Measures sampling adequacy for each variable and overall\")\nprint(f\"  Overall KMO: {kmo_model:.3f}\")\nif kmo_model > 0.8:\n    kmo_interpretation = \"Excellent\"\nelif kmo_model > 0.7:\n    kmo_interpretation = \"Good\"\nelif kmo_model > 0.6:\n    kmo_interpretation = \"Acceptable\"\nelse:\n    kmo_interpretation = \"Unacceptable\"\nprint(f\"  Interpretation: {kmo_interpretation} sampling adequacy\")\nprint()\n\nprint(\"Bartlett's Test of Sphericity:\")\nprint(\"  Tests null hypothesis: correlation matrix is identity matrix\")\nprint(f\"  Chi-square: {chi_square_value:.3f}, p-value: {p_value:.3f}\")\nif p_value < 0.05:\n    print(\"  Result: Significant - variables are correlated, FA is appropriate\")\nelse:\n    print(\"  Result: Not significant - variables may be uncorrelated, reconsider FA\")"
  },
  {
   "cell_type": "markdown",
   "id": "adb02449",
   "metadata": {},
   "source": [
    "## Step 4: Determine Number of Factors to Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c61155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Factor Retention Decision\n",
      "-------------------------\n",
      "Using Kaiser criterion (eigenvalues > 1.0):\n",
      "Eigenvalues: [1.298 1.074 0.987 0.932 0.758]\n",
      "Suggested number of factors: 2\n",
      "Rationale: Factors should explain more variance than individual variables\n"
     ]
    }
   ],
   "source": [
    "# Use PCA eigenvalues as initial guide for factor retention\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "eigenvalues = pca.explained_variance_\n",
    "\n",
    "# Kaiser criterion: retain factors with eigenvalues > 1.0\n",
    "n_factors = sum(eigenvalues > 1)\n",
    "\n",
    "print(\"Step 4: Factor Retention Decision\")\n",
    "print(\"-\" * 25)\n",
    "print(\"Using Kaiser criterion (eigenvalues > 1.0):\")\n",
    "print(f\"Eigenvalues: {np.round(eigenvalues, 3)}\")\n",
    "print(f\"Suggested number of factors: {n_factors}\")\n",
    "print(\"Rationale: Factors should explain more variance than individual variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9823e4",
   "metadata": {},
   "source": [
    "## Step 5: Perform Factor Analysis with Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8ee6da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Factor Extraction and Rotation\n",
      "-------------------------\n",
      "Method: Principal Axis Factoring with 2 factors\n",
      "Rotation: Varimax (orthogonal) for simple structure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julihocc/ma2003b/ma2003b.worktrees/dev/.venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Extract factors using Principal Axis Factoring with Varimax rotation\n",
    "fa = FactorAnalyzer(n_factors=n_factors, rotation=\"varimax\", method=\"principal\")\n",
    "fa.fit(X_scaled)\n",
    "\n",
    "print(\"Step 5: Factor Extraction and Rotation\")\n",
    "print(\"-\" * 25)\n",
    "print(f\"Method: Principal Axis Factoring with {n_factors} factors\")\n",
    "print(\"Rotation: Varimax (orthogonal) for simple structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a6ef8d",
   "metadata": {},
   "source": [
    "## Step 6: Extract and Interpret Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7688d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: Results Interpretation\n",
      "-------------------------\n",
      "\n",
      "Factor Loadings (Variable-Factor Correlations):\n",
      "  Values > 0.6 indicate strong factor relationships\n",
      "  Values > 0.3 indicate moderate relationships\n",
      "[[-0.778 -0.023]\n",
      " [ 0.366  0.613]\n",
      " [-0.229  0.832]\n",
      " [ 0.492 -0.098]\n",
      " [ 0.433  0.222]]\n",
      "\n",
      "Communalities (h² - Common Variance):\n",
      "  Proportion of variance explained by extracted factors\n",
      "  Higher values indicate better factor model fit\n",
      "[0.606 0.51  0.744 0.251 0.237]\n",
      "\n",
      "Factor Variance Explained:\n",
      "  [0]: Sum of squared loadings (eigenvalues)\n",
      "  [1]: Proportional variance explained\n",
      "  [2]: Cumulative variance explained\n",
      "[[1.222 1.127]\n",
      " [0.244 0.225]\n",
      " [0.244 0.47 ]]\n"
     ]
    }
   ],
   "source": [
    "# Get key factor analysis outputs\n",
    "loadings = fa.loadings_  # Variable-factor correlations\n",
    "communalities = fa.get_communalities()  # Common variance proportions\n",
    "variance_explained = fa.get_factor_variance()  # Variance decomposition\n",
    "\n",
    "print(\"Step 6: Results Interpretation\")\n",
    "print(\"-\" * 25)\n",
    "print()\n",
    "\n",
    "print(\"Factor Loadings (Variable-Factor Correlations):\")\n",
    "print(\"  Values > 0.6 indicate strong factor relationships\")\n",
    "print(\"  Values > 0.3 indicate moderate relationships\")\n",
    "if loadings is not None:\n",
    "    print(loadings.round(3))\n",
    "else:\n",
    "    print(\"  Loadings not available\")\n",
    "print()\n",
    "\n",
    "print(\"Communalities (h² - Common Variance):\")\n",
    "print(\"  Proportion of variance explained by extracted factors\")\n",
    "print(\"  Higher values indicate better factor model fit\")\n",
    "print(communalities.round(3))\n",
    "print()\n",
    "\n",
    "print(\"Factor Variance Explained:\")\n",
    "print(\"  [0]: Sum of squared loadings (eigenvalues)\")\n",
    "print(\"  [1]: Proportional variance explained\")\n",
    "print(\"  [2]: Cumulative variance explained\")\n",
    "print(np.round(variance_explained, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613f4d77",
   "metadata": {},
   "source": [
    "## Step 7: Compare with Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb086b18",
   "metadata": {},
   "outputs": [],
   "source": "# Transform data using both methods for comparison\npca_scores = pca.transform(X_scaled)[:, :n_factors]  # Keep same number of components\nfa_scores = fa.transform(X_scaled)\n\nprint(\"Step 7: PCA vs Factor Analysis Comparison\")\nprint(\"-\" * 25)\nprint()\n\n# Variance comparison\npca_variance = pca.explained_variance_ratio_[:n_factors].sum()\nfa_variance = variance_explained[2][-1]  # Cumulative variance from FA\n\nprint(\"Variance Explained Comparison:\")\nprint(f\"  PCA: {pca_variance*100:.1f}% cumulative variance\")\nprint(f\"  FA: {fa_variance*100:.1f}% cumulative variance\")\nprint()\n\nprint(\"Key Differences:\")\nprint(\"- PCA: Maximizes total variance, includes unique + common variance\")\nprint(\"- FA: Focuses on common variance, models unique variance separately\")\nprint(\"- PCA: Components are linear combinations for dimensionality reduction\")\nprint(\"- FA: Factors represent latent constructs for theory testing\")\nprint()\n\nprint(\"Factor Scores Shape:\")\nprint(f\"  PCA scores: {pca_scores.shape}\")\nprint(f\"  FA scores: {fa_scores.shape}\")\nprint(\"  Both provide component/factor scores for further analysis\")"
  },
  {
   "cell_type": "markdown",
   "id": "67ffcc02",
   "metadata": {},
   "source": [
    "## Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992917fd",
   "metadata": {},
   "outputs": [],
   "source": "print(\"Analysis Complete!\")\nprint(\"=\" * 50)\nprint(\"Summary:\")\nprint(f\"- Extracted {n_factors} factors from {X.shape[1]} variables\")\nprint(f\"- Cumulative variance explained: {fa_variance*100:.1f}%\")\nprint(f\"- KMO measure: {kmo_model:.3f}\")\n\nif kmo_model > 0.6 and p_value < 0.05:\n    print(\"- Data meets factorability requirements\")\nelse:\n    print(\"- Consider data quality issues or alternative methods\")\n\nprint(\"\\nNext Steps:\")\nprint(\"- Examine factor loadings for theoretical interpretation\")\nprint(\"- Consider oblique rotation if factors are expected to correlate\")\nprint(\"- Validate factor structure with confirmatory methods\")\nprint(\"- Use factor scores in subsequent analyses\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}