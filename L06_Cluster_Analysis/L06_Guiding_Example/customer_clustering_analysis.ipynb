{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-Commerce Customer Segmentation Using Cluster Analysis\n",
    "\n",
    "## Business Context\n",
    "\n",
    "**Business Problem:** An e-commerce company wants to understand their customer base by discovering natural segments based on purchasing behavior, engagement patterns, and browsing habits. Unlike classification problems where groups are predefined, we use unsupervised cluster analysis to discover these patterns.\n",
    "\n",
    "**Key Questions:**\n",
    "- How many distinct customer segments exist in our data?\n",
    "- What behavioral patterns characterize each segment?\n",
    "- How can we tailor marketing strategies to each discovered segment?\n",
    "\n",
    "**Dataset:** 2,000 customers with 7 behavioral features (no predefined labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading\n",
    "\n",
    "**Task:** Import required libraries, configure logging for transparency, and load the customer behavioral data. Display the first few rows and basic statistics to understand the data structure and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "logger.info(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load customer data (unsupervised - no labels)\n",
    "script_dir = Path.cwd()\n",
    "data_file = script_dir / \"customer_data.csv\"\n",
    "\n",
    "df = pd.read_csv(data_file)\n",
    "logger.info(f\"Loaded {len(df)} customer records\")\n",
    "\n",
    "print(\"\\n=== First 5 Customers ===\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n=== Dataset Summary ===\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "**Task:** Create visualizations to understand the distribution of each behavioral variable and check for correlations. This helps identify which features might drive customer segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots for each variable\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "fig.suptitle(\"Distribution of Customer Behavioral Variables\", fontsize=16)\n",
    "\n",
    "for idx, col in enumerate(df.columns):\n",
    "    row = idx // 3\n",
    "    col_idx = idx % 3\n",
    "    axes[row, col_idx].hist(df[col], bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[row, col_idx].set_title(col)\n",
    "    axes[row, col_idx].set_xlabel(\"Value\")\n",
    "    axes[row, col_idx].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(df.columns), 9):\n",
    "    row = idx // 3\n",
    "    col_idx = idx % 3\n",
    "    axes[row, col_idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"customer_distributions.png\", dpi=300, bbox_inches='tight')\n",
    "logger.info(\"Saved distribution plots to customer_distributions.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "plt.title(\"Correlation Matrix of Customer Behavioral Variables\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"correlation_matrix.png\", dpi=300, bbox_inches='tight')\n",
    "logger.info(\"Saved correlation matrix to correlation_matrix.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Strongest Correlations ===\")\n",
    "corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_pairs.append((\n",
    "            correlation_matrix.columns[i],\n",
    "            correlation_matrix.columns[j],\n",
    "            correlation_matrix.iloc[i, j]\n",
    "        ))\n",
    "\n",
    "corr_pairs_sorted = sorted(corr_pairs, key=lambda x: abs(x[2]), reverse=True)\n",
    "for var1, var2, corr in corr_pairs_sorted[:5]:\n",
    "    print(f\"{var1} <-> {var2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Standardization\n",
    "\n",
    "**Task:** Standardize all variables to have mean = 0 and standard deviation = 1. This is critical in cluster analysis because distance-based algorithms are sensitive to variable scales. Without standardization, variables with larger ranges would dominate the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(df)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "df_standardized = pd.DataFrame(X_standardized, columns=df.columns)\n",
    "\n",
    "logger.info(\"Data standardized: mean approx 0, std approx 1 for all variables\")\n",
    "\n",
    "print(\"\\n=== Standardized Data Summary ===\")\n",
    "print(df_standardized.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering Analysis\n",
    "\n",
    "**Task:** Apply hierarchical clustering using different linkage methods (single, complete, average, Ward's). Create dendrograms to visualize the clustering hierarchy and identify the optimal number of clusters by looking for large vertical gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute linkage matrices for different methods\n",
    "linkage_methods = ['single', 'complete', 'average', 'ward']\n",
    "linkage_matrices = {}\n",
    "\n",
    "for method in linkage_methods:\n",
    "    linkage_matrices[method] = linkage(X_standardized, method=method)\n",
    "    logger.info(f\"Computed {method} linkage\")\n",
    "\n",
    "# Create dendrograms for each method\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle(\"Dendrograms for Different Linkage Methods\", fontsize=16)\n",
    "\n",
    "for idx, method in enumerate(linkage_methods):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    \n",
    "    ax = axes[row, col]\n",
    "    dendrogram(linkage_matrices[method], ax=ax, no_labels=True)\n",
    "    ax.set_title(f\"{method.capitalize()} Linkage\")\n",
    "    ax.set_xlabel(\"Customer Index\")\n",
    "    ax.set_ylabel(\"Distance\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"dendrograms_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "logger.info(\"Saved dendrograms to dendrograms_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focused Dendrogram Analysis\n",
    "\n",
    "**Task:** Create a more detailed dendrogram using Ward's method (most common for customer segmentation) and identify the optimal cutting point. Ward's method minimizes within-cluster variance, producing compact, spherical clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Ward dendrogram\n",
    "plt.figure(figsize=(14, 7))\n",
    "dendrogram(linkage_matrices['ward'], no_labels=True, color_threshold=50)\n",
    "plt.title(\"Ward's Linkage Dendrogram (Detailed View)\", fontsize=14)\n",
    "plt.xlabel(\"Customer Index\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.axhline(y=50, color='r', linestyle='--', label='Potential cut (4 clusters)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ward_dendrogram_detailed.png\", dpi=300, bbox_inches='tight')\n",
    "logger.info(\"Saved detailed Ward dendrogram\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation: Look for large vertical gaps in the dendrogram.\")\n",
    "print(\"A horizontal cut at a large gap suggests a natural number of clusters.\")\n",
    "print(\"The red dashed line shows a potential cut point suggesting 4 clusters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Hierarchical Clusters\n",
    "\n",
    "**Task:** Cut the dendrogram to extract 4 clusters using Ward's method. Display the cluster sizes and calculate the silhouette score to measure cluster quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 4 clusters from Ward's linkage\n",
    "n_clusters_hier = 4\n",
    "hierarchical_labels = fcluster(linkage_matrices['ward'], n_clusters_hier, criterion='maxclust')\n",
    "\n",
    "# Convert to 0-indexed for consistency\n",
    "hierarchical_labels = hierarchical_labels - 1\n",
    "\n",
    "logger.info(f\"Extracted {n_clusters_hier} clusters using Ward's method\")\n",
    "\n",
    "print(\"\\n=== Hierarchical Clustering Results (Ward's Method) ===\")\n",
    "print(f\"Number of clusters: {n_clusters_hier}\")\n",
    "print(\"\\nCluster sizes:\")\n",
    "unique, counts = np.unique(hierarchical_labels, return_counts=True)\n",
    "for cluster_id, count in zip(unique, counts):\n",
    "    percentage = (count / len(hierarchical_labels)) * 100\n",
    "    print(f\"  Cluster {cluster_id}: {count} customers ({percentage:.1f}%)\")\n",
    "\n",
    "# Calculate silhouette score\n",
    "silhouette_hier = silhouette_score(X_standardized, hierarchical_labels)\n",
    "print(f\"\\nSilhouette Score: {silhouette_hier:.3f}\")\n",
    "print(\"(Range: -1 to +1, higher is better, >0.5 indicates good clustering)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering Analysis\n",
    "\n",
    "**Task:** Apply the elbow method to determine the optimal number of clusters for k-means. Plot the within-cluster sum of squares (inertia) for k = 2 to 10 and identify the \"elbow\" point where adding more clusters yields diminishing returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method to determine optimal k\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_standardized)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_standardized, kmeans.labels_))\n",
    "    logger.info(f\"K-means with k={k}: inertia={kmeans.inertia_:.2f}, silhouette={silhouette_scores[-1]:.3f}\")\n",
    "\n",
    "# Plot elbow curve\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(K_range, inertias, 'bo-')\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('Within-Cluster Sum of Squares (Inertia)')\n",
    "ax1.set_title('Elbow Method for Optimal k')\n",
    "ax1.grid(True)\n",
    "ax1.axvline(x=4, color='r', linestyle='--', label='Suggested k=4')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(K_range, silhouette_scores, 'go-')\n",
    "ax2.set_xlabel('Number of Clusters (k)')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Score vs Number of Clusters')\n",
    "ax2.grid(True)\n",
    "ax2.axvline(x=4, color='r', linestyle='--', label='Suggested k=4')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"elbow_silhouette_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "logger.info(\"Saved elbow and silhouette analysis plots\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Elbow plot: Look for the 'elbow' where the rate of decrease slows\")\n",
    "print(\"- Silhouette plot: Higher values indicate better-defined clusters\")\n",
    "print(f\"- Maximum silhouette score at k={K_range[np.argmax(silhouette_scores)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply K-Means with Optimal k\n",
    "\n",
    "**Task:** Run k-means clustering with k=4 (suggested by both hierarchical and elbow analyses). Display cluster sizes and compute the silhouette score to compare with hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply k-means with optimal k\n",
    "optimal_k = 4\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans_final.fit_predict(X_standardized)\n",
    "\n",
    "logger.info(f\"Applied k-means with k={optimal_k}\")\n",
    "\n",
    "print(\"\\n=== K-Means Clustering Results ===\")\n",
    "print(f\"Number of clusters: {optimal_k}\")\n",
    "print(\"\\nCluster sizes:\")\n",
    "unique, counts = np.unique(kmeans_labels, return_counts=True)\n",
    "for cluster_id, count in zip(unique, counts):\n",
    "    percentage = (count / len(kmeans_labels)) * 100\n",
    "    print(f\"  Cluster {cluster_id}: {count} customers ({percentage:.1f}%)\")\n",
    "\n",
    "# Calculate silhouette score\n",
    "silhouette_kmeans = silhouette_score(X_standardized, kmeans_labels)\n",
    "print(f\"\\nSilhouette Score: {silhouette_kmeans:.3f}\")\n",
    "\n",
    "print(\"\\n=== Comparison with Hierarchical Clustering ===\")\n",
    "print(f\"Hierarchical (Ward) silhouette: {silhouette_hier:.3f}\")\n",
    "print(f\"K-means silhouette: {silhouette_kmeans:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Interpretation and Profiling\n",
    "\n",
    "**Task:** For each cluster discovered by k-means, calculate the mean values of all behavioral variables (in original units, not standardized). Create a heatmap showing cluster profiles to identify distinguishing characteristics of each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster labels to original data\n",
    "df_with_clusters = df.copy()\n",
    "df_with_clusters['Cluster_KMeans'] = kmeans_labels\n",
    "df_with_clusters['Cluster_Hierarchical'] = hierarchical_labels\n",
    "\n",
    "# Calculate cluster means (in original units)\n",
    "cluster_profiles = df_with_clusters.groupby('Cluster_KMeans')[df.columns].mean()\n",
    "\n",
    "print(\"\\n=== K-Means Cluster Profiles (Mean Values) ===\")\n",
    "print(cluster_profiles.round(2))\n",
    "\n",
    "# Create heatmap of cluster profiles\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cluster_profiles.T, annot=True, fmt='.1f', cmap='YlOrRd', cbar_kws={'label': 'Average Value'})\n",
    "plt.title('K-Means Cluster Profiles Heatmap', fontsize=14)\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Behavioral Variable')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cluster_profiles_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "logger.info(\"Saved cluster profiles heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Cluster Characterization\n",
    "\n",
    "**Task:** For each cluster, identify the most distinctive characteristics by comparing each cluster's mean to the overall dataset mean. This helps assign business-meaningful names to clusters (e.g., 'High-Value Customers', 'Bargain Hunters')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare each cluster to overall means\n",
    "overall_means = df.mean()\n",
    "\n",
    "print(\"\\n=== Cluster Characterization (Comparison to Overall Means) ===\")\n",
    "for cluster_id in range(optimal_k):\n",
    "    print(f\"\\nCluster {cluster_id} (n={counts[cluster_id]}, {(counts[cluster_id]/len(df)*100):.1f}%):\")\n",
    "    cluster_mean = cluster_profiles.loc[cluster_id]\n",
    "    \n",
    "    # Find features significantly above or below average\n",
    "    differences = ((cluster_mean - overall_means) / overall_means * 100)\n",
    "    \n",
    "    # Show top distinguishing features\n",
    "    high_features = differences.nlargest(3)\n",
    "    low_features = differences.nsmallest(3)\n",
    "    \n",
    "    print(\"  Distinctive High Features:\")\n",
    "    for feat, diff in high_features.items():\n",
    "        if diff > 10:\n",
    "            print(f\"    - {feat}: {diff:+.1f}% vs average\")\n",
    "    \n",
    "    print(\"  Distinctive Low Features:\")\n",
    "    for feat, diff in low_features.items():\n",
    "        if diff < -10:\n",
    "            print(f\"    - {feat}: {diff:+.1f}% vs average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Validation with Silhouette Analysis\n",
    "\n",
    "**Task:** Create a silhouette plot showing the silhouette coefficient for each customer, grouped by cluster. This visualizes cluster cohesion (how well each point fits within its cluster) and separation (how distinct clusters are from each other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate silhouette values for each sample\n",
    "silhouette_vals = silhouette_samples(X_standardized, kmeans_labels)\n",
    "\n",
    "# Create silhouette plot\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "y_lower = 10\n",
    "for i in range(optimal_k):\n",
    "    # Get silhouette values for cluster i\n",
    "    cluster_silhouette_vals = silhouette_vals[kmeans_labels == i]\n",
    "    cluster_silhouette_vals.sort()\n",
    "    \n",
    "    size_cluster_i = cluster_silhouette_vals.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "    \n",
    "    color = plt.cm.tab10(i / optimal_k)\n",
    "    ax.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_vals,\n",
    "                     facecolor=color, edgecolor=color, alpha=0.7)\n",
    "    \n",
    "    # Label the silhouette plots with cluster numbers\n",
    "    ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "    \n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "ax.set_title('Silhouette Plot for K-Means Clustering', fontsize=14)\n",
    "ax.set_xlabel('Silhouette Coefficient')\n",
    "ax.set_ylabel('Cluster')\n",
    "ax.axvline(x=silhouette_kmeans, color=\"red\", linestyle=\"--\", label=f'Average: {silhouette_kmeans:.3f}')\n",
    "ax.set_yticks([])\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"silhouette_plot.png\", dpi=300, bbox_inches='tight')\n",
    "logger.info(\"Saved silhouette plot\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Width of silhouette plot = cluster size\")\n",
    "print(\"- Silhouette coefficient close to +1: well-matched to own cluster\")\n",
    "print(\"- Silhouette coefficient close to 0: on border between clusters\")\n",
    "print(\"- Silhouette coefficient close to -1: possibly assigned to wrong cluster\")\n",
    "print(\"- Red dashed line shows average silhouette score across all customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization in 2D Space (PCA Projection)\n",
    "\n",
    "**Task:** Since we cannot visualize 7-dimensional data directly, use Principal Component Analysis (PCA) to project the data onto the first two principal components. Plot customers colored by their cluster assignments to visualize cluster separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_standardized)\n",
    "\n",
    "logger.info(f\"PCA variance explained: PC1={pca.explained_variance_ratio_[0]:.3f}, PC2={pca.explained_variance_ratio_[1]:.3f}\")\n",
    "logger.info(f\"Total variance explained by 2 components: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "# Create scatter plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# K-means clusters\n",
    "scatter1 = ax1.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, cmap='tab10', alpha=0.6)\n",
    "ax1.scatter(pca.transform(kmeans_final.cluster_centers_)[:, 0], \n",
    "           pca.transform(kmeans_final.cluster_centers_)[:, 1],\n",
    "           marker='X', s=200, c='red', edgecolors='black', linewidths=2, label='Centroids')\n",
    "ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "ax1.set_title('K-Means Clustering (PCA Projection)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=ax1, label='Cluster')\n",
    "\n",
    "# Hierarchical clusters\n",
    "scatter2 = ax2.scatter(X_pca[:, 0], X_pca[:, 1], c=hierarchical_labels, cmap='tab10', alpha=0.6)\n",
    "ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "ax2.set_title('Hierarchical Clustering (PCA Projection)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=ax2, label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cluster_visualization_pca.png\", dpi=300, bbox_inches='tight')\n",
    "logger.info(\"Saved PCA cluster visualization\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nNote: This 2D projection captures {pca.explained_variance_ratio_.sum():.1%} of total variance.\")\n",
    "print(\"The actual clusters exist in 7-dimensional space and may be more separated than shown here.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Recommendations\n",
    "\n",
    "**Task:** Based on the cluster profiles, develop targeted marketing strategies for each customer segment. Consider the distinctive characteristics of each cluster when formulating recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BUSINESS RECOMMENDATIONS BY CLUSTER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nBased on cluster analysis, tailor marketing strategies as follows:\\n\")\n",
    "\n",
    "# Generic recommendations - instructor should customize based on actual cluster profiles\n",
    "recommendations = {\n",
    "    \"High-Spending, Frequent Shoppers\": [\n",
    "        \"Launch VIP loyalty program with exclusive perks\",\n",
    "        \"Offer early access to new products and sales\",\n",
    "        \"Provide free expedited shipping\",\n",
    "        \"Personalized product recommendations based on purchase history\"\n",
    "    ],\n",
    "    \"Price-Sensitive, Small-Basket Buyers\": [\n",
    "        \"Send targeted discount codes and bundle offers\",\n",
    "        \"Promote free shipping thresholds to increase basket size\",\n",
    "        \"Highlight clearance and sale items in email campaigns\",\n",
    "        \"Create value packs and multi-buy promotions\"\n",
    "    ],\n",
    "    \"Browsers with Low Conversion\": [\n",
    "        \"Implement abandoned cart recovery campaigns\",\n",
    "        \"Use retargeting ads to re-engage visitors\",\n",
    "        \"Offer limited-time discounts to incentivize first purchase\",\n",
    "        \"Improve product information and customer reviews to build trust\"\n",
    "    ],\n",
    "    \"Sporadic High-Value Buyers\": [\n",
    "        \"Send promotional emails highlighting new arrivals and special offers\",\n",
    "        \"Create urgency with flash sales and limited-time deals\",\n",
    "        \"Implement win-back campaigns during inactive periods\",\n",
    "        \"Offer flexible return policies to reduce purchase anxiety\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for segment_name, strategies in recommendations.items():\n",
    "    print(f\"\\n{segment_name}:\")\n",
    "    for strategy in strategies:\n",
    "        print(f\"  - {strategy}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Validate cluster assignments with domain experts\")\n",
    "print(\"2. Implement targeted campaigns for each segment\")\n",
    "print(\"3. Monitor segment-specific KPIs (conversion, AOV, retention)\")\n",
    "print(\"4. Re-run clustering periodically to detect shifts in customer behavior\")\n",
    "print(\"5. Consider additional features (geographic, demographic) for refinement\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with True Labels (Validation Only)\n",
    "\n",
    "**Task:** Load the validation dataset (which contains true cluster labels from data generation) to assess how well our unsupervised clustering recovered the underlying structure. Calculate the Adjusted Rand Index to measure agreement between discovered and true clusters.\n",
    "\n",
    "**Important Note:** In real-world unsupervised learning, true labels do not exist. This validation step is only possible because we generated synthetic data with known structure. In practice, cluster validation relies on domain expertise and business metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, confusion_matrix\n",
    "\n",
    "# Load validation file with true labels\n",
    "validation_file = script_dir / \"customer_data_with_labels.csv\"\n",
    "\n",
    "if validation_file.exists():\n",
    "    df_validation = pd.read_csv(validation_file)\n",
    "    true_labels = df_validation['true_cluster']\n",
    "    \n",
    "    # Calculate Adjusted Rand Index\n",
    "    ari_kmeans = adjusted_rand_score(true_labels, kmeans_labels)\n",
    "    ari_hierarchical = adjusted_rand_score(true_labels, hierarchical_labels)\n",
    "    \n",
    "    print(\"\\n=== Validation Against True Clusters (Educational Purpose Only) ===\")\n",
    "    print(f\"K-means Adjusted Rand Index: {ari_kmeans:.3f}\")\n",
    "    print(f\"Hierarchical Adjusted Rand Index: {ari_hierarchical:.3f}\")\n",
    "    print(\"\\n(ARI ranges from -1 to 1, where 1 = perfect match, 0 = random)\")\n",
    "    \n",
    "    # Show true cluster distribution\n",
    "    print(\"\\n=== True Cluster Distribution ===\")\n",
    "    print(true_labels.value_counts().sort_index())\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(true_labels, kmeans_labels)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=[f'K{i}' for i in range(optimal_k)],\n",
    "                yticklabels=sorted(true_labels.unique()))\n",
    "    plt.title('Confusion Matrix: True vs K-Means Clusters')\n",
    "    plt.xlabel('K-Means Cluster')\n",
    "    plt.ylabel('True Cluster')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"validation_confusion_matrix.png\", dpi=300, bbox_inches='tight')\n",
    "    logger.info(\"Saved validation confusion matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nNote: This validation is only possible because we generated synthetic data.\")\n",
    "    print(\"In real applications, there are no 'true' labels to compare against.\")\n",
    "else:\n",
    "    print(\"\\nValidation file not found. Skipping true label comparison.\")\n",
    "    print(\"Run fetch_customer_data.py to generate the validation dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "**Task:** Summarize the entire cluster analysis workflow, key findings, and methodological insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLUSTER ANALYSIS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. METHODOLOGY\")\n",
    "print(\"   - Analyzed 2,000 customers across 7 behavioral variables\")\n",
    "print(\"   - Standardized data to ensure equal feature weighting\")\n",
    "print(\"   - Applied both hierarchical (Ward's linkage) and k-means clustering\")\n",
    "print(\"   - Used elbow method, silhouette analysis, and dendrogram inspection\")\n",
    "\n",
    "print(\"\\n2. OPTIMAL NUMBER OF CLUSTERS\")\n",
    "print(f\"   - Both methods suggested k = {optimal_k} clusters\")\n",
    "print(f\"   - K-means silhouette score: {silhouette_kmeans:.3f}\")\n",
    "print(f\"   - Hierarchical silhouette score: {silhouette_hier:.3f}\")\n",
    "\n",
    "print(\"\\n3. CLUSTER SIZES (K-Means)\")\n",
    "for cluster_id, count in zip(unique, counts):\n",
    "    percentage = (count / len(kmeans_labels)) * 100\n",
    "    print(f\"   - Cluster {cluster_id}: {count} customers ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\n4. KEY INSIGHTS\")\n",
    "print(\"   - Distinct customer segments exist with different behavioral patterns\")\n",
    "print(\"   - Segments vary in spending levels, purchase frequency, and engagement\")\n",
    "print(\"   - Each segment requires tailored marketing strategies\")\n",
    "\n",
    "print(\"\\n5. METHODOLOGICAL LEARNINGS\")\n",
    "print(\"   - Hierarchical clustering: Provides hierarchy view, no need to predefine k\")\n",
    "print(\"   - K-means: More scalable, faster, but requires specifying k\")\n",
    "print(\"   - Both methods converged on similar cluster structure\")\n",
    "print(\"   - Standardization is critical when variables have different scales\")\n",
    "\n",
    "print(\"\\n6. BUSINESS VALUE\")\n",
    "print(\"   - Enables targeted marketing campaigns by segment\")\n",
    "print(\"   - Optimizes resource allocation to high-value customers\")\n",
    "print(\"   - Identifies opportunities for customer retention and growth\")\n",
    "print(\"   - Provides data-driven customer understanding\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "logger.info(\"Cluster analysis completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
