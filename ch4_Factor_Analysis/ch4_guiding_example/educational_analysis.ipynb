{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Factor Analysis vs PCA: Educational Assessment Data\n",
    "\n",
    "This analysis demonstrates both Factor Analysis (FA) and Principal Component Analysis (PCA)\n",
    "on student assessment data to identify latent constructs and understand dimensionality.\n",
    "\n",
    "**Learning objectives:**\n",
    "- Apply Factor Analysis to discover latent psychological constructs\n",
    "- Apply PCA to identify underlying dimensions in multivariate data\n",
    "- Understand communalities and uniquenesses in measurement models\n",
    "- Use factor rotation to achieve simple structure\n",
    "- Compare Factor Analysis with Principal Component Analysis\n",
    "- Interpret factor loadings for construct validation\n",
    "- Understand the fundamental differences between FA and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## Import Libraries and Setup\n",
    "\n",
    "**Task:** Set up your Python environment for both Factor Analysis and PCA by importing the necessary libraries. You'll need:\n",
    "- pandas for data handling\n",
    "- numpy for numerical operations\n",
    "- matplotlib and seaborn for visualization\n",
    "- scikit-learn's PCA and StandardScaler\n",
    "- factor_analyzer package (including FactorAnalyzer, calculate_kmo, and calculate_bartlett_sphericity)\n",
    "- Configure a basic logger for tracking analysis steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Simple logger\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration\n",
    "\n",
    "**Task:** Load the educational assessment data from the CSV file in the current directory. Check if the file exists (if not, inform the user to run the fetch script and exit with code 1). Extract the assessment variables (excluding the Student ID column) and log basic information about the dataset dimensions and variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = Path.cwd()\n",
    "data_path = script_dir / \"educational.csv\"\n",
    "\n",
    "if not data_path.exists():\n",
    "    logger.error(f\"Data file not found: {data_path}\")\n",
    "    logger.info(\"Run 'fetch_educational.py' to generate the required data file\")\n",
    "    sys.exit(1)\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "logger.info(\n",
    "    f\"Loaded dataset: {len(df)} students, {len(df.columns) - 1} assessment variables\"\n",
    ")\n",
    "\n",
    "# Extract assessment variables (exclude Student ID)\n",
    "X = df.iloc[:, 1:]\n",
    "variable_names = list(X.columns)\n",
    "\n",
    "logger.info(f\"Assessment variables: {variable_names}\")\n",
    "logger.info(f\"Data shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standardize",
   "metadata": {},
   "source": [
    "## Data Standardization\n",
    "\n",
    "**Task:** Standardize the assessment data using StandardScaler so all variables have mean 0 and standard deviation 1. Log a confirmation message after standardization.\n",
    "\n",
    "Both Factor Analysis and PCA require standardized data to ensure variables contribute equally\n",
    "to the analysis, regardless of their original measurement scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standardize_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "logger.info(\"Data standardized: mean ≈ 0, std ≈ 1 for all variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumptions",
   "metadata": {},
   "source": [
    "## Factor Analysis Assumptions Testing\n",
    "\n",
    "**Task:** Test the statistical assumptions for Factor Analysis by calculating Bartlett's Test of Sphericity and the Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy. Log the chi-square statistic and p-value for Bartlett's test (with interpretation whether p < 0.05). For KMO, log the overall measure and classify it as Excellent (>0.9), Good (>0.8), Acceptable (>0.6), or Unacceptable.\n",
    "\n",
    "Before proceeding with Factor Analysis, we must verify that our data meets\n",
    "key statistical assumptions for meaningful factor extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumptions_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test statistical assumptions\n",
    "chi_square_value, p_value = calculate_bartlett_sphericity(X_standardized)\n",
    "kmo_all, kmo_model = calculate_kmo(X_standardized)\n",
    "\n",
    "logger.info(\"Factor Analysis Assumptions Testing:\")\n",
    "logger.info(\"\\nBartlett's Test of Sphericity:\")\n",
    "logger.info(f\"  Chi-square statistic: {chi_square_value:.3f}\")\n",
    "logger.info(f\"  p-value: {p_value:.6f}\")\n",
    "if p_value < 0.05:\n",
    "    logger.info(\"  ✓ Significant - variables are sufficiently correlated for FA\")\n",
    "else:\n",
    "    logger.info(\"  ✗ Not significant - FA may not be appropriate\")\n",
    "\n",
    "logger.info(\"\\nKaiser-Meyer-Olkin (KMO) Test:\")\n",
    "logger.info(f\"  Overall Measure of Sampling Adequacy: {kmo_model:.3f}\")\n",
    "if kmo_model > 0.9:\n",
    "    adequacy = \"Excellent\"\n",
    "elif kmo_model > 0.8:\n",
    "    adequacy = \"Good\"\n",
    "elif kmo_model > 0.6:\n",
    "    adequacy = \"Acceptable\"\n",
    "else:\n",
    "    adequacy = \"Unacceptable\"\n",
    "logger.info(f\"  Interpretation: {adequacy} sampling adequacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual_adequacy",
   "metadata": {},
   "source": [
    "### Individual Variable Adequacy\n",
    "\n",
    "**Task:** Extract and log the individual KMO (Measure of Sampling Adequacy) value for each variable. Identify any variables with MSA < 0.6 and flag them as problematic, or confirm that all variables show adequate sampling adequacy.\n",
    "\n",
    "Each variable's individual KMO value indicates how well it can be predicted\n",
    "from the other variables in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual_adequacy_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"\\nIndividual Variable Sampling Adequacy:\")\n",
    "for i, var_name in enumerate(variable_names):\n",
    "    msa_value = kmo_all[i]\n",
    "    logger.info(f\"  {var_name}: {msa_value:.3f}\")\n",
    "\n",
    "# Flag any problematic variables\n",
    "low_msa_vars = [\n",
    "    var_name for i, var_name in enumerate(variable_names) if kmo_all[i] < 0.6\n",
    "]\n",
    "if low_msa_vars:\n",
    "    logger.warning(f\"Variables with low MSA (<0.6): {low_msa_vars}\")\n",
    "else:\n",
    "    logger.info(\"All variables show adequate sampling adequacy (MSA ≥ 0.6)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa_extraction",
   "metadata": {},
   "source": [
    "## Factor Extraction with Principal Axis Factoring\n",
    "\n",
    "**Task:** Extract 3 factors using Principal Axis Factoring (method='principal') without rotation. Use the FactorAnalyzer class, fit it to the standardized data, and verify that the extraction succeeded by checking if loadings were produced. Log the eigenvalues for the extracted factors rounded to 3 decimal places.\n",
    "\n",
    "We'll extract factors using Principal Axis Factoring (PAF), which:\n",
    "- Focuses on shared variance among variables (common factors)\n",
    "- Estimates communalities iteratively\n",
    "- Distinguishes between common and unique variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa_extraction_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine number of factors to extract\n",
    "n_factors = 3  # Based on theoretical expectation of quantitative, verbal, and interpersonal factors\n",
    "\n",
    "fa_unrotated = FactorAnalyzer(n_factors=n_factors, rotation=None, method=\"principal\")\n",
    "fa_unrotated.fit(X_standardized)\n",
    "\n",
    "# Verify successful extraction\n",
    "if fa_unrotated.loadings_ is None:\n",
    "    logger.error(\"Factor extraction failed - no loadings produced\")\n",
    "    sys.exit(1)\n",
    "\n",
    "logger.info(f\"Factor Analysis Results ({n_factors} factors extracted):\")\n",
    "eigenvalues_fa = fa_unrotated.get_eigenvalues()[0]\n",
    "logger.info(f\"Eigenvalues: {np.round(eigenvalues_fa[:n_factors], 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communalities",
   "metadata": {},
   "source": [
    "### Communalities and Variance Decomposition\n",
    "\n",
    "**Task:** Extract the communalities from the factor analysis and calculate uniquenesses (1 - communality). For each variable, log its communality (h²) and uniqueness (u²) values rounded to 3 decimal places. Then calculate and log the total common variance (sum of communalities), the proportion of total variance explained by factors, and the average communality.\n",
    "\n",
    "Each variable's variance is decomposed into:\n",
    "- **Communality (h²)**: Variance explained by common factors\n",
    "- **Uniqueness (u²)**: Variance unique to the variable (including error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communalities_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "communalities = fa_unrotated.get_communalities()\n",
    "uniquenesses = 1 - communalities\n",
    "\n",
    "logger.info(\"\\nVariance Decomposition (Communalities and Uniquenesses):\")\n",
    "for i, var_name in enumerate(variable_names):\n",
    "    h2 = communalities[i]\n",
    "    u2 = uniquenesses[i]\n",
    "    logger.info(f\"  {var_name}: h² = {h2:.3f}, u² = {u2:.3f}\")\n",
    "\n",
    "# Analyze overall variance structure\n",
    "factor_variance = np.sum(communalities)\n",
    "total_variance = len(variable_names)  # For standardized data\n",
    "variance_explained_fa = factor_variance / total_variance\n",
    "\n",
    "logger.info(\"\\nOverall Variance Analysis:\")\n",
    "logger.info(f\"Total standardized variance: {total_variance:.1f}\")\n",
    "logger.info(f\"Common variance (Σh²): {factor_variance:.3f}\")\n",
    "logger.info(f\"Proportion of variance explained by factors: {variance_explained_fa:.1%}\")\n",
    "logger.info(f\"Average communality: {np.mean(communalities):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communalities_interpretation",
   "metadata": {},
   "source": [
    "**Interpreting Communalities:**\n",
    "\n",
    "- **High communality (h² > 0.6)**: Variable strongly related to common factors\n",
    "- **Moderate communality (0.3 < h² < 0.6)**: Moderate factor relationship\n",
    "- **Low communality (h² < 0.3)**: Mostly unique variance, weak factor loading\n",
    "\n",
    "All nine assessment variables show high communalities, indicating they are well-explained\n",
    "by the three underlying educational constructs (Quantitative, Verbal, Interpersonal)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1532b76",
   "metadata": {},
   "source": [
    "## Inspect and Interpret Unrotated Factors\n",
    "\n",
    "**Task:** Before applying rotation, examine and interpret the unrotated factor solution. Display the unrotated loadings matrix as a DataFrame with proper variable and factor labels. Analyze the loading patterns to identify: (1) what each factor represents, (2) problems with interpretability (general factors, bipolar contrasts, lack of simple structure), and (3) why rotation is needed to achieve theory-aligned, interpretable factors.\n",
    "\n",
    "Understanding the unrotated solution reveals why rotation is essential for achieving simple structure and practical interpretability in factor analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pur2ws18pzf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the unrotated loadings from the fitted model\n",
    "loadings_unrotated = fa_unrotated.loadings_\n",
    "\n",
    "# Create a detailed DataFrame of unrotated loadings\n",
    "unrotated_loadings_df = pd.DataFrame(\n",
    "    loadings_unrotated,\n",
    "    columns=[f\"Factor {i+1}\" for i in range(n_factors)],\n",
    "    index=variable_names\n",
    ")\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*70)\n",
    "logger.info(\"UNROTATED FACTOR LOADINGS ANALYSIS\")\n",
    "logger.info(\"=\"*70)\n",
    "\n",
    "print(\"\\nUnrotated Factor Loadings Matrix:\")\n",
    "print(unrotated_loadings_df.round(3))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oll6sgbxun",
   "metadata": {},
   "source": [
    "### Interpretation of Unrotated Factor Loadings\n",
    "\n",
    "Looking at the unrotated loadings table above, we can identify three distinct patterns:\n",
    "\n",
    "#### **Factor 1: General Educational Ability (The \"g-factor\")**\n",
    "- **ALL variables** load positively and moderately high (0.577 to 0.737)\n",
    "- This is the classic \"general factor\" pattern commonly seen in educational and psychological research\n",
    "- **Interpretation:** Students who score high on Factor 1 tend to score high on ALL assessments across domains\n",
    "- **Problem:** This doesn't help us understand specific abilities - it represents overall academic competence rather than distinct constructs\n",
    "- **Not actionable:** We can't say \"this student is strong in Factor 1\" and know what specific skills they have\n",
    "\n",
    "#### **Factor 2: Interpersonal vs. Cognitive Skills Contrast**\n",
    "- **Positive loadings:** Collaboration (0.706), Leadership (0.710), Communication (0.650)\n",
    "- **Negative loadings:** Math (-0.478), Algebra (-0.438), Geometry (-0.464), Reading (-0.175), Vocabulary (-0.153), Writing (-0.097)\n",
    "- **Interpretation:** This is a **bipolar contrast dimension**\n",
    "  - High Factor 2 score = Strong interpersonal skills relative to cognitive skills\n",
    "  - Low Factor 2 score = Strong cognitive skills relative to interpersonal skills\n",
    "- **Problem:** The bipolar nature creates confusion. Does a high score mean:\n",
    "  - Good at interpersonal skills?\n",
    "  - Bad at math/reading?\n",
    "  - Both?\n",
    "\n",
    "#### **Factor 3: Quantitative vs. Verbal Contrast**\n",
    "- **Positive loadings:** Math (0.422), Algebra (0.555), Geometry (0.338)\n",
    "- **Negative loadings:** Reading (-0.527), Vocabulary (-0.588), Writing (-0.558)\n",
    "- **Weak/Mixed:** Interpersonal skills (0.107 to 0.212)\n",
    "- **Interpretation:** Another **bipolar contrast dimension**\n",
    "  - High Factor 3 = Stronger quantitative skills relative to verbal skills\n",
    "  - Low Factor 3 = Stronger verbal skills relative to quantitative skills\n",
    "- **Problem:** This suggests students can't be good at BOTH math and reading, which doesn't align with reality or theory\n",
    "\n",
    "### **Key Problems with Unrotated Solution:**\n",
    "\n",
    "**Problem 1: Lack of Simple Structure**\n",
    "   - Most variables have substantial loadings on multiple factors\n",
    "   - Example: MathScore loads 0.687 on F1, -0.478 on F2, and 0.422 on F3\n",
    "   - Makes it unclear which factor \"owns\" each variable\n",
    "\n",
    "**Problem 2: Difficult Substantive Interpretation**\n",
    "   - What does \"high on Factor 1\" really mean beyond \"generally capable\"?\n",
    "   - Bipolar factors create ambiguous interpretations\n",
    "\n",
    "**Problem 3: Bipolar Factors Create Confusion**\n",
    "   - Negative loadings suggest inverse relationships that may not be theoretically meaningful\n",
    "   - \"Good at X but bad at Y\" is harder to interpret than \"good at X\"\n",
    "\n",
    "**Problem 4: Not Theory-Aligned**\n",
    "   - We theoretically expect 3 **independent, positive** constructs:\n",
    "     - Quantitative Reasoning (Math, Algebra, Geometry)\n",
    "     - Verbal Ability (Reading, Vocabulary, Writing)\n",
    "     - Interpersonal Skills (Collaboration, Leadership, Communication)\n",
    "   - Unrotated solution gives us contrasts and a general factor instead\n",
    "\n",
    "**Problem 5: Low Practical Utility**\n",
    "   - Cannot easily create subscales or interpret student profiles\n",
    "   - Difficult to communicate results to educators or stakeholders\n",
    "\n",
    "### **Why These Patterns Emerge:**\n",
    "\n",
    "The unrotated solution is mathematically optimal for **variance extraction**:\n",
    "- Factor 1 extracts maximum variance (eigenvalue = 4.041)\n",
    "- Factor 2 extracts maximum remaining variance (eigenvalue = 2.124)\n",
    "- Factor 3 extracts maximum remaining variance (eigenvalue = 1.635)\n",
    "\n",
    "However, **maximum variance does not equal maximum interpretability**. The unrotated factors are orthogonal (uncorrelated) but arranged to maximize variance extraction, not psychological meaning.\n",
    "\n",
    "### **What We Need: Rotation**\n",
    "\n",
    "Rotation will transform these factors to achieve:\n",
    "\n",
    "**Benefit 1: Simple Structure** - Each variable loads primarily on ONE factor\n",
    "\n",
    "**Benefit 2: Clear Interpretation** - Factors represent distinct, interpretable constructs\n",
    "\n",
    "**Benefit 3: Elimination of Bipolar Factors** - All loadings become predominantly positive\n",
    "\n",
    "**Benefit 4: Theory Alignment** - Factors match expected Quantitative, Verbal, and Interpersonal dimensions\n",
    "\n",
    "**Benefit 5: Practical Utility** - Easy to create subscales and interpret student profiles\n",
    "\n",
    "**Important:** Rotation does NOT change:\n",
    "- The communalities (h²) for each variable\n",
    "- The total variance explained\n",
    "- The fundamental fit of the model\n",
    "- The orthogonality of factors (for Varimax rotation)\n",
    "\n",
    "Rotation is simply a **rigid transformation** of the factor space to achieve better interpretability while preserving all mathematical properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotation",
   "metadata": {},
   "source": [
    "## Factor Rotation for Simple Structure\n",
    "\n",
    "**Task:** Extract 3 factors again using Principal Axis Factoring but this time with Varimax rotation. Store both the unrotated and rotated loadings matrices. Include a safety check in case rotation fails (use unrotated solution as fallback). Create a comparison DataFrame showing the unrotated and rotated loadings side-by-side for all three factors, and display it rounded to 3 decimal places.\n",
    "\n",
    "Factor rotation improves interpretability without changing the fundamental solution.\n",
    "Varimax rotation seeks \"simple structure\" where each variable loads primarily on one factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotation_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_rotated = FactorAnalyzer(n_factors=n_factors, rotation=\"varimax\", method=\"principal\")\n",
    "fa_rotated.fit(X_standardized)\n",
    "\n",
    "loadings_unrotated = fa_unrotated.loadings_\n",
    "loadings_rotated = fa_rotated.loadings_\n",
    "\n",
    "# Safety check for rotation success\n",
    "if loadings_rotated is None:\n",
    "    logger.warning(\"Varimax rotation failed, using unrotated solution\")\n",
    "    loadings_rotated = loadings_unrotated\n",
    "    fa_rotated = fa_unrotated\n",
    "\n",
    "logger.info(\"\\nFactor Loadings Comparison (Unrotated vs. Varimax Rotated):\")\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Variable\": variable_names,\n",
    "        \"Unrot_F1\": loadings_unrotated[:, 0],\n",
    "        \"Unrot_F2\": loadings_unrotated[:, 1],\n",
    "        \"Unrot_F3\": loadings_unrotated[:, 2],\n",
    "        \"Rotated_F1\": loadings_rotated[:, 0],\n",
    "        \"Rotated_F2\": loadings_rotated[:, 1],\n",
    "        \"Rotated_F3\": loadings_rotated[:, 2],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(comparison_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotation_interpretation",
   "metadata": {},
   "source": [
    "### Factor Loading Interpretation\n",
    "\n",
    "**Rotation benefits:**\n",
    "- **Simple structure**: Variables load primarily on one factor\n",
    "- **Clearer interpretation**: Easier to identify what each factor represents\n",
    "- **Practical meaning**: Factors align better with theoretical constructs\n",
    "\n",
    "**Loading interpretation guidelines:**\n",
    "- **|loading| > 0.6**: Strong factor relationship\n",
    "- **0.3 < |loading| < 0.6**: Moderate relationship\n",
    "- **|loading| < 0.3**: Weak/negligible relationship\n",
    "\n",
    "**Expected factor structure after rotation:**\n",
    "- **Factor 1**: Likely Interpersonal Skills (Collaboration, Leadership, Communication)\n",
    "- **Factor 2**: Likely Verbal Ability (ReadingComp, Vocabulary, Writing)\n",
    "- **Factor 3**: Likely Quantitative Reasoning (MathScore, AlgebraScore, GeometryScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pca_analysis",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "**Task:** Apply PCA to the standardized data and transform it to obtain principal component scores. Extract the eigenvalues, explained variance ratios, and cumulative variance. Log these results rounded to 3 decimal places to understand how much variance each component captures.\n",
    "\n",
    "PCA identifies linear combinations of variables that capture maximum variance.\n",
    "Unlike Factor Analysis, PCA does not distinguish between common and unique variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca_analysis_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "Z = pca.fit_transform(X_standardized)\n",
    "\n",
    "eigenvalues_pca = pca.explained_variance_\n",
    "explained_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_ratio)\n",
    "\n",
    "logger.info(\"\\nPCA Results:\")\n",
    "logger.info(f\"Eigenvalues: {np.round(eigenvalues_pca, 3)}\")\n",
    "logger.info(f\"Explained variance ratio: {np.round(explained_ratio, 3)}\")\n",
    "logger.info(f\"Cumulative variance: {np.round(cumulative_variance, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pca_interpretation",
   "metadata": {},
   "source": [
    "### Interpreting the Variance Structure\n",
    "\n",
    "The eigenvalues and explained variance ratios reveal the underlying dimensionality:\n",
    "\n",
    "**Key insights:**\n",
    "- **PC1, PC2, PC3**: Each captures a major dimension of educational assessment\n",
    "- **First 3 components**: Should explain >80% cumulative variance based on the three underlying factors\n",
    "- **Later components**: Capture measurement noise or minor variations\n",
    "\n",
    "**Component retention strategy:**\n",
    "- Kaiser criterion: Retain components with eigenvalues > 1.0 (expect 3 components)\n",
    "- Scree plot: Look for the \"elbow\" where eigenvalues level off\n",
    "- Practical rule: Retain components explaining ≥80% cumulative variance\n",
    "\n",
    "**Expected interpretation:**\n",
    "- **PC1**: Likely represents general cognitive ability or interpersonal skills\n",
    "- **PC2**: Likely distinguishes verbal vs. quantitative reasoning\n",
    "- **PC3**: Likely captures a specific educational dimension or mixed skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scree_comparison",
   "metadata": {},
   "source": [
    "## Eigenvalue Comparison: FA vs PCA\n",
    "\n",
    "**Task:** Create two side-by-side scree plots comparing PCA eigenvalues (left, in steelblue) with FA eigenvalues (right, in darkgreen). For each plot, show eigenvalues as connected points, add a horizontal line at eigenvalue = 1.0 (Kaiser criterion), include proper axis labels and titles, and add a grid. Save the combined figure as 'fa_scree.png'.\n",
    "\n",
    "Scree plots reveal how eigenvalues differ between methods due to their\n",
    "different approaches to variance decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scree_comparison_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "components = np.arange(1, len(eigenvalues_pca) + 1)\n",
    "\n",
    "# PCA scree plot\n",
    "ax1.plot(\n",
    "    components,\n",
    "    eigenvalues_pca,\n",
    "    \"o-\",\n",
    "    linewidth=2,\n",
    "    color=\"steelblue\",\n",
    "    markersize=8,\n",
    "    label=\"PCA Eigenvalues\",\n",
    ")\n",
    "ax1.axhline(y=1.0, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Kaiser criterion\")\n",
    "ax1.set_xlabel(\"Component Number\")\n",
    "ax1.set_ylabel(\"Eigenvalue\")\n",
    "ax1.set_title(\"PCA Eigenvalues\")\n",
    "ax1.set_xticks(components)\n",
    "ax1.grid(True, linestyle=\":\", alpha=0.7)\n",
    "ax1.legend()\n",
    "\n",
    "# FA scree plot\n",
    "ax2.plot(\n",
    "    components,\n",
    "    eigenvalues_fa,\n",
    "    \"o-\",\n",
    "    linewidth=2,\n",
    "    color=\"darkgreen\",\n",
    "    markersize=8,\n",
    "    label=\"FA Eigenvalues\",\n",
    ")\n",
    "ax2.axhline(y=1.0, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Kaiser criterion\")\n",
    "ax2.set_xlabel(\"Factor Number\")\n",
    "ax2.set_ylabel(\"Eigenvalue\")\n",
    "ax2.set_title(\"Factor Analysis Eigenvalues\")\n",
    "ax2.set_xticks(components)\n",
    "ax2.grid(True, linestyle=\":\", alpha=0.7)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "scree_path = script_dir / \"fa_scree.png\"\n",
    "plt.savefig(scree_path, dpi=150, bbox_inches=\"tight\")\n",
    "logger.info(f\"Eigenvalue comparison saved: {scree_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scree_interpretation",
   "metadata": {},
   "source": [
    "**Eigenvalue pattern differences:**\n",
    "\n",
    "- **FA eigenvalues**: Generally lower because they reflect only common variance\n",
    "- **PCA eigenvalues**: Higher because they include both common and unique variance\n",
    "- **Kaiser criterion**: Both methods should show 3 eigenvalues > 1.0, confirming the 3-factor structure\n",
    "- **Elbow location**: Clear drop after the 3rd component indicates optimal retention of 3 factors\n",
    "- **Theoretical focus**: FA prioritizes meaningful factors over variance maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loadings_viz",
   "metadata": {},
   "source": [
    "## Factor Loading Visualization\n",
    "\n",
    "**Task:** Create a side-by-side comparison of two heatmaps showing unrotated and Varimax rotated factor loadings. Use matplotlib's imshow with the 'RdBu_r' colormap (range -1 to 1). Label the x-axis with variable names (rotated 45 degrees), y-axis with factor numbers. Add text annotations showing the exact loading values (rounded to 3 decimals) in each cell. Include a shared colorbar on the right side of the figure. Save the figure as 'fa_loadings.png'.\n",
    "\n",
    "Heatmaps provide visual comparison of loading patterns before and after rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loadings_viz_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Unrotated loadings heatmap using matplotlib imshow\n",
    "im1 = ax1.imshow(\n",
    "    loadings_unrotated.T,\n",
    "    cmap=\"RdBu_r\",\n",
    "    aspect=\"auto\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ")\n",
    "ax1.set_title(\"Unrotated Factor Loadings\")\n",
    "ax1.set_xlabel(\"Variables\")\n",
    "ax1.set_ylabel(\"Factors\")\n",
    "ax1.set_xticks(range(len(variable_names)))\n",
    "ax1.set_xticklabels(variable_names, rotation=45, ha=\"right\")\n",
    "ax1.set_yticks(range(n_factors))\n",
    "ax1.set_yticklabels([f\"Factor {i + 1}\" for i in range(n_factors)])\n",
    "\n",
    "# Add annotations\n",
    "for i in range(n_factors):\n",
    "    for j in range(len(variable_names)):\n",
    "        text = ax1.text(\n",
    "            j,\n",
    "            i,\n",
    "            f\"{loadings_unrotated[j, i]:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"black\",\n",
    "            fontsize=7,\n",
    "        )\n",
    "\n",
    "# Rotated loadings heatmap using matplotlib imshow\n",
    "im2 = ax2.imshow(\n",
    "    loadings_rotated.T,\n",
    "    cmap=\"RdBu_r\",\n",
    "    aspect=\"auto\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ")\n",
    "ax2.set_title(\"Varimax Rotated Factor Loadings\")\n",
    "ax2.set_xlabel(\"Variables\")\n",
    "ax2.set_ylabel(\"Factors\")\n",
    "ax2.set_xticks(range(len(variable_names)))\n",
    "ax2.set_xticklabels(variable_names, rotation=45, ha=\"right\")\n",
    "ax2.set_yticks(range(n_factors))\n",
    "ax2.set_yticklabels([f\"Factor {i + 1}\" for i in range(n_factors)])\n",
    "\n",
    "# Add annotations\n",
    "for i in range(n_factors):\n",
    "    for j in range(len(variable_names)):\n",
    "        text = ax2.text(\n",
    "            j,\n",
    "            i,\n",
    "            f\"{loadings_rotated[j, i]:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"black\",\n",
    "            fontsize=7,\n",
    "        )\n",
    "\n",
    "# Adjust layout and add colorbar to the right\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.85)  # Make room for colorbar\n",
    "cbar_ax = fig.add_axes([0.87, 0.15, 0.02, 0.7])  # Position for colorbar\n",
    "cbar = fig.colorbar(im1, cax=cbar_ax)\n",
    "cbar.set_label(\"Loading\")\n",
    "\n",
    "loadings_path = script_dir / \"fa_loadings.png\"\n",
    "plt.savefig(loadings_path, dpi=150, bbox_inches=\"tight\")\n",
    "logger.info(f\"Factor loadings heatmap saved: {loadings_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biplot",
   "metadata": {},
   "source": [
    "## PCA Biplot Visualization\n",
    "\n",
    "**Task:** Create a PCA biplot combining student scores on PC1 and PC2 (as colored scatter points) with variable loadings (as red arrows emanating from the origin). Scale the loading arrows appropriately so they're visible on the same plot. Color the points by their PC1 score using a colormap. Include axis labels showing the variance explained by each component. Save the plot as 'pca_biplot.png'.\n",
    "\n",
    "The biplot combines:\n",
    "- **Points**: Individual student scores on PC1 and PC2\n",
    "- **Arrows**: Variable loadings showing how each assessment contributes to the components\n",
    "\n",
    "**Interpretation guide:**\n",
    "- Arrow direction indicates which component the variable loads on\n",
    "- Arrow length indicates loading strength\n",
    "- Similar arrows suggest variables measure related constructs\n",
    "- Opposite arrows indicate negative correlation\n",
    "- Clusters of related variables (Quantitative, Verbal, Interpersonal) should be evident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biplot_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Plot student scores\n",
    "pc1_scores = Z[:, 0]\n",
    "pc2_scores = Z[:, 1]\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    pc1_scores,\n",
    "    pc2_scores,\n",
    "    c=pc1_scores,\n",
    "    cmap=\"viridis\",\n",
    "    alpha=0.6,\n",
    "    s=40,\n",
    "    edgecolors=\"black\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "colorbar = plt.colorbar(scatter, label=\"PC1 Score\")\n",
    "\n",
    "# Plot variable loadings as arrows\n",
    "pca_loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "scale_factor = max(pc1_scores.std(), pc2_scores.std()) * 3.5\n",
    "\n",
    "for i, var_name in enumerate(variable_names):\n",
    "    loading_x = pca_loadings[i, 0] / np.sqrt(pca.explained_variance_[0]) * scale_factor\n",
    "    loading_y = pca_loadings[i, 1] / np.sqrt(pca.explained_variance_[1]) * scale_factor\n",
    "\n",
    "    ax.arrow(\n",
    "        0,\n",
    "        0,\n",
    "        loading_x,\n",
    "        loading_y,\n",
    "        color=\"red\",\n",
    "        head_width=0.15,\n",
    "        alpha=0.8,\n",
    "        linewidth=2.5,\n",
    "        head_length=0.15,\n",
    "    )\n",
    "    ax.text(\n",
    "        loading_x * 1.15,\n",
    "        loading_y * 1.15,\n",
    "        var_name,\n",
    "        color=\"red\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=11,\n",
    "        ha=\"center\",\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(f\"PC1 ({explained_ratio[0]:.1%} of variance)\")\n",
    "ax.set_ylabel(f\"PC2 ({explained_ratio[1]:.1%} of variance)\")\n",
    "ax.set_title(\"PCA Biplot: Student Scores and Variable Loadings\")\n",
    "ax.grid(True, linestyle=\":\", alpha=0.3)\n",
    "ax.axhline(y=0, color=\"black\", linewidth=0.8)\n",
    "ax.axvline(x=0, color=\"black\", linewidth=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "biplot_path = script_dir / \"pca_biplot.png\"\n",
    "plt.savefig(biplot_path, dpi=150, bbox_inches=\"tight\")\n",
    "logger.info(f\"Biplot saved: {biplot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa_vs_pca",
   "metadata": {},
   "source": [
    "## Factor Analysis vs PCA Comparison\n",
    "\n",
    "**Task:** Compare the variance explanation: calculate the proportion of total variance explained by the first 3 PCA components vs. the proportion of common variance explained by 3 FA factors. Get factor scores from the rotated FA model. Create a comparison DataFrame showing PCA loadings (PC1, PC2, PC3) and FA loadings (F1, F2, F3) side-by-side for all variables. Display rounded to 3 decimals.\n",
    "\n",
    "Direct comparison reveals fundamental differences between these two approaches\n",
    "to multivariate data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa_vs_pca_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get factor scores\n",
    "fa_scores = fa_rotated.transform(X_standardized)\n",
    "\n",
    "logger.info(\"\\nFactor Analysis vs PCA Comparison:\")\n",
    "\n",
    "# Variance explanation comparison\n",
    "pca_variance_3comp = explained_ratio[:3].sum()\n",
    "logger.info(\"\\nVariance Explanation:\")\n",
    "logger.info(f\"  PCA (first 3 components): {pca_variance_3comp:.1%} of total variance\")\n",
    "logger.info(f\"  FA (3 factors): {variance_explained_fa:.1%} of common variance\")\n",
    "logger.info(\n",
    "    \"  Key difference: PCA maximizes total variance, FA focuses on shared variance\"\n",
    ")\n",
    "\n",
    "# Loading comparison\n",
    "logger.info(\"\\nLoading Pattern Comparison:\")\n",
    "comparison_detailed = pd.DataFrame(\n",
    "    {\n",
    "        \"Variable\": variable_names,\n",
    "        \"PCA_PC1\": pca_loadings[:, 0],\n",
    "        \"PCA_PC2\": pca_loadings[:, 1],\n",
    "        \"PCA_PC3\": pca_loadings[:, 2],\n",
    "        \"FA_F1\": loadings_rotated[:, 0],\n",
    "        \"FA_F2\": loadings_rotated[:, 1],\n",
    "        \"FA_F3\": loadings_rotated[:, 2],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(comparison_detailed.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "factor_interpretation",
   "metadata": {},
   "source": [
    "## Factor Structure Interpretation and Validation\n",
    "\n",
    "**Task:** Analyze the rotated factor structure by identifying salient loadings (absolute value > 0.4) for each factor. Log which variables load strongly on each factor with their loading values. Interpret the factor structure in terms of the three educational constructs.\n",
    "\n",
    "Analyze the extracted factors to understand what psychological constructs\n",
    "they represent and how well they align with theoretical expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "factor_interpretation_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify salient loadings (commonly |loading| > 0.4)\n",
    "loading_threshold = 0.4\n",
    "\n",
    "logger.info(f\"\\nFactor Structure Analysis (threshold = {loading_threshold}):\")\n",
    "for factor_idx in range(n_factors):\n",
    "    factor_name = f\"Factor {factor_idx + 1}\"\n",
    "    salient_vars = []\n",
    "\n",
    "    for var_idx, var_name in enumerate(variable_names):\n",
    "        loading = loadings_rotated[var_idx, factor_idx]\n",
    "        if abs(loading) > loading_threshold:\n",
    "            salient_vars.append(f\"{var_name} ({loading:+.3f})\")\n",
    "\n",
    "    logger.info(\n",
    "        f\"  {factor_name}: {', '.join(salient_vars) if salient_vars else 'No salient loadings'}\"\n",
    "    )\n",
    "\n",
    "logger.info(\"\\nEducational Construct Interpretation:\")\n",
    "logger.info(\"  Factor 1: Likely represents Interpersonal Skills\")\n",
    "logger.info(\"    - Expected high loadings: Collaboration, Leadership, Communication\")\n",
    "logger.info(\"  Factor 2: Likely represents Verbal Ability\")\n",
    "logger.info(\"    - Expected high loadings: ReadingComp, Vocabulary, Writing\")\n",
    "logger.info(\"  Factor 3: Likely represents Quantitative Reasoning\")\n",
    "logger.info(\"    - Expected high loadings: MathScore, AlgebraScore, GeometryScore\")\n",
    "logger.info(\"\\nAll variables should show high communalities (h² > 0.6),\")\n",
    "logger.info(\"indicating they are well-explained by the three underlying factors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pca_loadings",
   "metadata": {},
   "source": [
    "## PCA Component Loadings Analysis\n",
    "\n",
    "**Task:** Create a DataFrame showing the PCA loadings (component coefficients) for the first 3 principal components. Use the variable names as row labels and 'PC1', 'PC2', 'PC3' as column names. Display the loadings table rounded to 3 decimal places.\n",
    "\n",
    "Loadings show how each variable contributes to each principal component.\n",
    "High absolute loadings indicate strong relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca_loadings_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loadings table for first 3 components\n",
    "pca_loadings_df = pd.DataFrame(\n",
    "    pca.components_[:3].T, columns=[\"PC1\", \"PC2\", \"PC3\"], index=variable_names\n",
    ")\n",
    "\n",
    "logger.info(\"\\nPCA Component Loadings Matrix:\")\n",
    "print(pca_loadings_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "student_scores",
   "metadata": {},
   "source": [
    "## Student Score Analysis\n",
    "\n",
    "**Task:** Create a DataFrame containing student IDs (using the actual STUD_xxx format from the data) with their PC1 and PC2 scores. Identify and log the top 5 students with the highest scores on PC1, showing both their PC1 and PC2 values rounded to 3 decimal places.\n",
    "\n",
    "Examine how students rank on the principal components to understand\n",
    "the practical meaning of each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "student_scores_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create student ranking analysis using actual Student IDs from data\n",
    "student_scores = pd.DataFrame(\n",
    "    {\"Student_ID\": df[\"Student\"].values, \"PC1_Score\": Z[:, 0], \"PC2_Score\": Z[:, 1]}\n",
    ")\n",
    "\n",
    "# Top performers on PC1\n",
    "top_pc1 = student_scores.nlargest(5, \"PC1_Score\")\n",
    "logger.info(\"\\nTop 5 students on PC1:\")\n",
    "for _, row in top_pc1.iterrows():\n",
    "    logger.info(\n",
    "        f\"  {row['Student_ID']}: PC1={row['PC1_Score']:.3f}, PC2={row['PC2_Score']:.3f}\"\n",
    "    )\n",
    "\n",
    "# Top performers on PC2\n",
    "top_pc2 = student_scores.nlargest(5, \"PC2_Score\")\n",
    "logger.info(\"\\nTop 5 students on PC2:\")\n",
    "for _, row in top_pc2.iterrows():\n",
    "    logger.info(\n",
    "        f\"  {row['Student_ID']}: PC1={row['PC1_Score']:.3f}, PC2={row['PC2_Score']:.3f}\"\n",
    "    )\n",
    "\n",
    "# Score distribution summary\n",
    "logger.info(\"\\nScore Distribution Summary:\")\n",
    "logger.info(f\"  PC1 range: [{Z[:, 0].min():.3f}, {Z[:, 0].max():.3f}]\")\n",
    "logger.info(f\"  PC2 range: [{Z[:, 1].min():.3f}, {Z[:, 1].max():.3f}]\")\n",
    "logger.info(\n",
    "    f\"  PC1-PC2 correlation: {np.corrcoef(Z[:, 0], Z[:, 1])[0, 1]:.3f} (should be ≈ 0)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary and Method Selection Guidelines\n",
    "\n",
    "This comparative analysis demonstrates key concepts for dimensionality reduction and latent variable modeling:\n",
    "\n",
    "**Factor Analysis advantages:**\n",
    "- **Theoretical grounding**: Models specific latent constructs (quantitative, verbal, interpersonal)\n",
    "- **Measurement model**: Separates common variance from measurement error\n",
    "- **Simple structure**: Rotation achieves cleaner variable-factor relationships\n",
    "- **Communality estimates**: Reveals how much variance is shared vs. unique\n",
    "- **Three-factor structure**: Clearly identifies Quantitative, Verbal, and Interpersonal dimensions\n",
    "\n",
    "**PCA advantages:**\n",
    "- **Maximum variance**: Captures the most information in each component\n",
    "- **Data reduction**: Efficient compression for visualization and further analysis\n",
    "- **Computational simplicity**: Faster and more stable than iterative FA\n",
    "- **Total variance**: Includes all sources of variation (common + unique)\n",
    "\n",
    "**When to choose Factor Analysis:**\n",
    "- Testing specific theories about latent psychological constructs\n",
    "- Developing or validating measurement instruments\n",
    "- Modeling common variance while acknowledging measurement error\n",
    "- Interpreting results in terms of theoretical constructs\n",
    "- Educational assessment scenarios where underlying abilities are of interest\n",
    "\n",
    "**When to choose PCA instead:**\n",
    "- Primary goal is data reduction or compression\n",
    "- Maximizing explained variance is the priority\n",
    "- No theoretical model for underlying structure\n",
    "- Computational efficiency is critical\n",
    "\n",
    "**Key methodological insights:**\n",
    "- **Assumption testing**: KMO and Bartlett's tests confirm FA appropriateness\n",
    "- **Rotation benefits**: Varimax rotation dramatically improves interpretability in FA\n",
    "- **Communality interpretation**: High communalities (h² > 0.6) for all variables confirm the data fits the 3-factor model\n",
    "- **Variance focus**: PCA explains more total variance, FA focuses on shared variance\n",
    "- **Construct validation**: Loading patterns should align with theoretical expectations\n",
    "- **Three-factor solution**: Kaiser criterion and scree plot both support 3 factors\n",
    "\n",
    "**Educational assessment insights:**\n",
    "- **Quantitative Reasoning**: Math, Algebra, and Geometry form a coherent dimension\n",
    "- **Verbal Ability**: Reading, Vocabulary, and Writing cluster together\n",
    "- **Interpersonal Skills**: Collaboration, Leadership, and Communication represent a distinct dimension\n",
    "- **Realistic correlations**: Factors show moderate intercorrelations (0.2-0.3) as expected in educational data\n",
    "- **Assessment design**: Results validate that these nine measures capture three distinct educational constructs\n",
    "\n",
    "**Next applications:**\n",
    "- Explore oblique rotation when factors may be correlated\n",
    "- Use confirmatory factor analysis to test specific theoretical models\n",
    "- Apply discriminant analysis for classification tasks\n",
    "- Combine with cluster analysis to identify student groups\n",
    "- Use canonical correlation to relate assessment domains\n",
    "- Develop composite scores for each factor for student reporting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
