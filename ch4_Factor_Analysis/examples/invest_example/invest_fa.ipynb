{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b04da3",
   "metadata": {},
   "source": [
    "# Factor Analysis of European Stock Markets\n",
    "\n",
    "This script applies Factor Analysis to European stock market data to identify\n",
    "common market factors and understand the latent structure of financial market\n",
    "integration. Unlike PCA which focuses on variance maximization, Factor Analysis\n",
    "seeks to model the shared comovement among European markets.\n",
    "\n",
    "## Financial Factor Analysis Applications:\n",
    "- **Market Integration**: Common factors representing systematic market risks\n",
    "- **Contagion Analysis**: How shocks spread through interconnected markets\n",
    "- **Portfolio Construction**: Risk factor identification for diversification\n",
    "- **Asset Pricing**: Multi-factor models (Fama-French, APT theory)\n",
    "\n",
    "## Expected Factor Structure:\n",
    "- **Common European Factor**: Shared economic/political influences\n",
    "- **Regional Factors**: Country-specific or sector-specific factors\n",
    "- **Idiosyncratic Components**: Market-specific movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa9967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c161c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging and paths\n",
    "script_dir = Path(__file__).resolve().parent\n",
    "\n",
    "# Simple logger\n",
    "import logging\n",
    "logger = logging.getLogger(\"invest_fa\")\n",
    "\n",
    "logger.info(\"Starting European Stock Markets Factor Analysis\")\n",
    "print(\"European Stock Markets - Factor Analysis\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837c4659",
   "metadata": {},
   "source": [
    "## Load and Examine Financial Data\n",
    "\n",
    "We use the same European stock market data (DAX, SMI, CAC, FTSE) to enable\n",
    "direct comparison with the PCA analysis. This dataset represents 1,860 trading\n",
    "days of market index returns, providing a rich basis for factor analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857903e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the investment data\n",
    "data_file = script_dir / \"invest.csv\"\n",
    "if not data_file.exists():\n",
    "    print(f\"Data file not found: {data_file}\")\n",
    "    print(\n",
    "        \"Please run: .venv/bin/python lessons/4_Factor_Analysis/code/invest_example/fetch_invest.py\"\n",
    "    )\n",
    "    exit(1)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(data_file, index_col=0)\n",
    "print(f\"Data loaded: {df.shape[0]} trading days × {df.shape[1]} market indices\")\n",
    "print(f\"Trading day range: Day {df.index.min()} to Day {df.index.max()}\")\n",
    "print(\"\\nMarket indices:\", list(df.columns))\n",
    "\n",
    "# Convert prices to returns for financial analysis\n",
    "print(\"\\nConverting price levels to daily returns...\")\n",
    "returns_df = df.pct_change().dropna()  # Daily percentage returns\n",
    "print(f\"Returns data: {returns_df.shape[0]} observations after conversion\")\n",
    "\n",
    "# Show basic statistics\n",
    "print(\"\\nMarket Statistics (Daily Returns):\")\n",
    "print(f\"{'Market':<8} {'Mean':<8} {'Std':<8} {'Min':<8} {'Max':<8}\")\n",
    "print(\"-\" * 44)\n",
    "for col in returns_df.columns:\n",
    "    returns_data = returns_df[col].dropna()\n",
    "    mean_ret = returns_data.mean()  # Daily return\n",
    "    volatility = returns_data.std()  # Daily volatility\n",
    "    print(\n",
    "        f\"{col:<8} {mean_ret:<8.1%} {volatility:<8.1%} {returns_data.min():<8.1%} {returns_data.max():<8.1%}\"\n",
    "    )\n",
    "\n",
    "# Update df to use returns for factor analysis\n",
    "df = returns_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8876560a",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Factor Analysis Assumptions\n",
    "\n",
    "Financial returns often require careful preprocessing:\n",
    "- **Stationarity**: Returns are typically stationary (prices are not)\n",
    "- **Standardization**: Equal weight to different volatility markets\n",
    "- **Missing values**: Handle non-synchronous trading days\n",
    "- **Outliers**: Consider impact of market crashes/rallies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a91979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values and prepare data for analysis\n",
    "X = df.values\n",
    "X_clean = X[~np.isnan(X).any(axis=1)]  # Remove rows with any NaN\n",
    "print(f\"Data after cleaning: {X_clean.shape[0]} complete observations\")\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_clean)\n",
    "\n",
    "# Check Factor Analysis assumptions\n",
    "chi_square_value, p_value = calculate_bartlett_sphericity(X_scaled)\n",
    "kmo_all, kmo_model = calculate_kmo(X_scaled)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FACTOR ANALYSIS ASSUMPTIONS\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Bartlett's Test of Sphericity:\")\n",
    "print(f\"  Chi-square: {chi_square_value:.2f}\")\n",
    "print(f\"  p-value: {p_value:.2e}\")\n",
    "print(\n",
    "    f\"  Result: {'✓ Suitable for FA' if p_value < 0.05 else '✗ May not be suitable for FA'}\"\n",
    ")\n",
    "\n",
    "print(\"\\nKaiser-Meyer-Olkin (KMO) Test:\")\n",
    "print(f\"  Overall MSA: {kmo_model:.3f}\")\n",
    "interpretation = (\n",
    "    \"✓ Excellent\"\n",
    "    if kmo_model > 0.9\n",
    "    else (\n",
    "        \"✓ Good\"\n",
    "        if kmo_model > 0.8\n",
    "        else (\n",
    "            \"✓ Acceptable\"\n",
    "            if kmo_model > 0.6\n",
    "            else \"⚠ Poor\"\n",
    "            if kmo_model > 0.5\n",
    "            else \"✗ Unacceptable\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "print(f\"  Interpretation: {interpretation} sampling adequacy\")\n",
    "\n",
    "print(\"\\nIndividual Market MSA Values:\")\n",
    "print(f\"{'Market':<8} {'MSA':<8}\")\n",
    "print(\"-\" * 18)\n",
    "for i, market in enumerate(df.columns):\n",
    "    print(f\"{market:<8} {kmo_all[i]:<8.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea757d",
   "metadata": {},
   "source": [
    "## Factor Extraction: Determine Number of Factors\n",
    "\n",
    "For financial markets, we use multiple criteria to determine factors:\n",
    "- **Kaiser Criterion**: Eigenvalues > 1\n",
    "- **Scree Plot**: Visual identification of \"elbow\"\n",
    "- **Theoretical Expectation**: 1-2 common market factors expected\n",
    "- **Parallel Analysis**: Compare with random data eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5ca47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform initial factor analysis to examine eigenvalues\n",
    "fa_explore = FactorAnalyzer(n_factors=df.shape[1], rotation=None, method=\"principal\")\n",
    "fa_explore.fit(X_scaled)\n",
    "\n",
    "eigenvalues, _ = fa_explore.get_eigenvalues()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FACTOR RETENTION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Factor':<8} {'Eigenvalue':<12} {'% Variance':<12} {'Cumulative %':<12}\")\n",
    "print(\"-\" * 48)\n",
    "\n",
    "cumulative_var = 0\n",
    "n_factors_kaiser = 0\n",
    "for i, eigenval in enumerate(eigenvalues):\n",
    "    var_explained = eigenval / len(df.columns) * 100\n",
    "    cumulative_var += var_explained\n",
    "    if eigenval > 1.0:\n",
    "        n_factors_kaiser += 1\n",
    "\n",
    "    print(\n",
    "        f\"Factor {i + 1:<2} {eigenval:<12.3f} {var_explained:<12.1f} {cumulative_var:<12.1f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nFactor Retention Criteria:\")\n",
    "print(f\"  Kaiser criterion (eigenvalue > 1): {n_factors_kaiser} factors\")\n",
    "print(\n",
    "    f\"  70% variance rule: {np.argmax(np.cumsum(eigenvalues) / np.sum(eigenvalues) >= 0.70) + 1} factors\"\n",
    ")\n",
    "print(\"  Financial theory expectation: 1-2 common market factors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648a8437",
   "metadata": {},
   "source": [
    "## Factor Analysis: Two-Factor Solution\n",
    "\n",
    "Based on financial theory and our factor retention analysis, we extract 2 factors:\n",
    "- **Factor 1**: Expected to be a general European market factor\n",
    "- **Factor 2**: Expected to capture regional/sectoral differences\n",
    "\n",
    "We compare unrotated and Varimax-rotated solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 2-factor solution\n",
    "n_factors = 2\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"FACTOR ANALYSIS: {n_factors}-FACTOR SOLUTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Unrotated solution\n",
    "fa_unrotated = FactorAnalyzer(n_factors=n_factors, rotation=None, method=\"principal\")\n",
    "fa_unrotated.fit(X_scaled)\n",
    "\n",
    "# Varimax rotated solution\n",
    "fa_rotated = FactorAnalyzer(n_factors=n_factors, rotation=\"varimax\", method=\"principal\")\n",
    "fa_rotated.fit(X_scaled)\n",
    "\n",
    "# Extract results\n",
    "loadings_unrotated = getattr(fa_unrotated, \"loadings_\", None)\n",
    "loadings_rotated = getattr(fa_rotated, \"loadings_\", None)\n",
    "\n",
    "# If unrotated loadings are not provided by the FactorAnalyzer implementation,\n",
    "# fall back to the rotated loadings for display purposes to avoid subscripting None.\n",
    "if loadings_unrotated is None:\n",
    "    logger.warning(\"Unrotated loadings are None; falling back to rotated loadings for display.\")\n",
    "    loadings_unrotated = loadings_rotated\n",
    "\n",
    "# Ensure we have loadings to work with\n",
    "if loadings_unrotated is None or loadings_rotated is None:\n",
    "    raise RuntimeError(\n",
    "        \"Factor loadings are unavailable (None). Ensure FactorAnalyzer.fit() succeeded \"\n",
    "        \"and the installed 'factor_analyzer' package exposes '.loadings_' after fit.\"\n",
    "    )\n",
    "\n",
    "communalities = fa_rotated.get_communalities()\n",
    "uniquenesses = 1 - communalities\n",
    "\n",
    "print(\"Factor Analysis Results:\")\n",
    "print(\n",
    "    f\"{'Market':<8} {'h²':<8} {'u²':<8} {'Unrot-F1':<10} {'Unrot-F2':<10} {'Vmax-F1':<10} {'Vmax-F2':<10}\"\n",
    ")\n",
    "print(\"-\" * 78)\n",
    "for i, market in enumerate(df.columns):\n",
    "    print(\n",
    "        f\"{market:<8} {communalities[i]:<8.3f} {uniquenesses[i]:<8.3f} \"\n",
    "        f\"{loadings_unrotated[i, 0]:<10.3f} {loadings_unrotated[i, 1]:<10.3f} \"\n",
    "        f\"{loadings_rotated[i, 0]:<10.3f} {loadings_rotated[i, 1]:<10.3f}\"\n",
    "    )\n",
    "\n",
    "# Calculate variance explained by factors\n",
    "total_communality = np.sum(communalities)\n",
    "proportion_common_variance = total_communality / len(df.columns)\n",
    "print(\"\\nVariance Analysis:\")\n",
    "print(f\"  Total communality (sum of h²): {total_communality:.3f}\")\n",
    "print(\n",
    "    f\"  Proportion of variance explained by factors: {proportion_common_variance:.1%}\"\n",
    ")\n",
    "print(f\"  Average communality per market: {np.mean(communalities):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3cf76e",
   "metadata": {},
   "source": [
    "## Factor Interpretation and Financial Insights\n",
    "\n",
    "Let's interpret what each factor represents in financial terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5395e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FACTOR INTERPRETATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Identify factor characteristics based on loadings\n",
    "loading_threshold = 0.4\n",
    "print(f\"Factor loadings above {loading_threshold} threshold:\")\n",
    "\n",
    "for factor_idx in range(n_factors):\n",
    "    factor_name = f\"Factor {factor_idx + 1}\"\n",
    "    high_loading_markets = []\n",
    "\n",
    "    print(f\"\\n{factor_name}:\")\n",
    "    for market_idx, market in enumerate(df.columns):\n",
    "        loading = loadings_rotated[market_idx, factor_idx]\n",
    "        if abs(loading) > loading_threshold:\n",
    "            sign = \"+\" if loading > 0 else \"-\"\n",
    "            high_loading_markets.append(f\"{sign}{market}({abs(loading):.2f})\")\n",
    "\n",
    "        print(f\"  {market:<8}: {loading:>6.3f}\")\n",
    "\n",
    "    if high_loading_markets:\n",
    "        print(f\"  High loadings: {', '.join(high_loading_markets)}\")\n",
    "\n",
    "    # Financial interpretation\n",
    "    if factor_idx == 0:\n",
    "        print(\"  → Likely represents: Common European market factor\")\n",
    "        print(\"    (Systematic risk affecting all markets)\")\n",
    "    elif factor_idx == 1:\n",
    "        print(\"  → Likely represents: Regional/sectoral differentiation\")\n",
    "        print(\"    (Idiosyncratic movements between markets)\")\n",
    "\n",
    "# Market integration analysis\n",
    "print(\"\\nMarket Integration Analysis:\")\n",
    "well_explained = [\n",
    "    market\n",
    "    for i, market in enumerate(df.columns)\n",
    "    if communalities[i] > np.mean(communalities)\n",
    "]\n",
    "poorly_explained = [\n",
    "    market\n",
    "    for i, market in enumerate(df.columns)\n",
    "    if communalities[i] <= np.mean(communalities)\n",
    "]\n",
    "\n",
    "print(f\"  Highly integrated markets (h² > average): {', '.join(well_explained)}\")\n",
    "print(f\"  More idiosyncratic markets (h² ≤ average): {', '.join(poorly_explained)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e679ef3",
   "metadata": {},
   "source": [
    "## Visualization: Factor Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ed7f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive factor analysis visualizations\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Factor loadings heatmap comparison\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "sns.heatmap(\n",
    "    loadings_unrotated.T,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    xticklabels=df.columns,\n",
    "    yticklabels=[f\"Factor {i + 1}\" for i in range(n_factors)],\n",
    "    cmap=\"RdYlGn_r\",\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    cbar_kws={\"shrink\": 0.8},\n",
    ")\n",
    "ax1.set_title(\"Unrotated Factor Loadings\")\n",
    "\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "sns.heatmap(\n",
    "    loadings_rotated.T,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    xticklabels=df.columns,\n",
    "    yticklabels=[f\"Factor {i + 1}\" for i in range(n_factors)],\n",
    "    cmap=\"RdYlGn_r\",\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    cbar_kws={\"shrink\": 0.8},\n",
    ")\n",
    "ax2.set_title(\"Varimax Rotated Loadings\")\n",
    "\n",
    "# 2. Communalities bar chart\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "bars = ax3.bar(df.columns, communalities, color=\"steelblue\", alpha=0.7)\n",
    "ax3.set_title(\"Communalities by Market\")\n",
    "ax3.set_ylabel(\"h² (Proportion of Variance Explained)\")\n",
    "ax3.set_ylim(0, 1)\n",
    "ax3.tick_params(axis=\"x\", rotation=45)\n",
    "ax3.axhline(\n",
    "    y=float(np.mean(communalities)),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Average = {np.mean(communalities):.3f}\",\n",
    ")\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Scree plot\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "factors = np.arange(1, len(eigenvalues) + 1)\n",
    "ax4.plot(factors, eigenvalues, \"o-\", color=\"steelblue\", markersize=8, linewidth=2)\n",
    "ax4.axhline(y=1.0, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Kaiser criterion\")\n",
    "ax4.set_xlabel(\"Factor Number\")\n",
    "ax4.set_ylabel(\"Eigenvalue\")\n",
    "ax4.set_title(\"Scree Plot\")\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.legend()\n",
    "ax4.set_xticks(factors)\n",
    "\n",
    "# 4. Factor scores scatter plot (first 100 observations)\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "factor_scores = fa_rotated.transform(X_scaled)\n",
    "scatter = ax5.scatter(\n",
    "    factor_scores[:100, 0],\n",
    "    factor_scores[:100, 1],\n",
    "    c=np.arange(100),\n",
    "    cmap=\"viridis\",\n",
    "    alpha=0.6,\n",
    ")\n",
    "ax5.set_xlabel(\"Factor 1 (Common Market)\")\n",
    "ax5.set_ylabel(\"Factor 2 (Regional Differences)\")\n",
    "ax5.set_title(\"Factor Scores (First 100 Days)\")\n",
    "ax5.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax5, label=\"Trading Day\")\n",
    "\n",
    "# 5. Uniquenesses vs Communalities\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "x = np.arange(len(df.columns))\n",
    "width = 0.35\n",
    "ax6.bar(\n",
    "    x - width / 2,\n",
    "    communalities,\n",
    "    width,\n",
    "    label=\"Communalities (h²)\",\n",
    "    color=\"steelblue\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "ax6.bar(\n",
    "    x + width / 2,\n",
    "    uniquenesses,\n",
    "    width,\n",
    "    label=\"Uniquenesses (u²)\",\n",
    "    color=\"lightcoral\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "ax6.set_xlabel(\"Markets\")\n",
    "ax6.set_ylabel(\"Proportion of Variance\")\n",
    "ax6.set_title(\"Variance Decomposition\")\n",
    "ax6.set_xticks(x)\n",
    "ax6.set_xticklabels(df.columns, rotation=45)\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "loadings_out = script_dir / \"invest_fa_loadings.png\"\n",
    "plt.savefig(loadings_out, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"\\nSaved comprehensive factor analysis plots: {loadings_out}\")\n",
    "logger.info(f\"Saved factor analysis visualization: {loadings_out}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e697c534",
   "metadata": {},
   "source": [
    "## Financial Risk and Portfolio Implications\n",
    "\n",
    "Factor Analysis results have direct applications in finance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082d6644",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FINANCIAL APPLICATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Systematic vs Idiosyncratic Risk\n",
    "systematic_risk = np.mean(communalities)  # Average explained by common factors\n",
    "idiosyncratic_risk = np.mean(uniquenesses)  # Average unexplained (market-specific)\n",
    "\n",
    "print(\"Risk Decomposition (Average across markets):\")\n",
    "print(f\"  Systematic risk (common factors): {systematic_risk:.1%}\")\n",
    "print(f\"  Idiosyncratic risk (market-specific): {idiosyncratic_risk:.1%}\")\n",
    "\n",
    "# 2. Market Factor Sensitivities\n",
    "print(\"\\nMarket Factor Sensitivities:\")\n",
    "print(f\"{'Market':<8} {'Factor 1':<10} {'Factor 2':<10} {'Interpretation'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, market in enumerate(df.columns):\n",
    "    f1_loading = loadings_rotated[i, 0]\n",
    "    f2_loading = loadings_rotated[i, 1]\n",
    "\n",
    "    # Interpret factor sensitivity\n",
    "    if abs(f1_loading) > abs(f2_loading):\n",
    "        interpretation = \"Common factor driven\"\n",
    "    else:\n",
    "        interpretation = \"Regional factor driven\"\n",
    "\n",
    "    print(f\"{market:<8} {f1_loading:<10.3f} {f2_loading:<10.3f} {interpretation}\")\n",
    "\n",
    "# 3. Portfolio Diversification Insights\n",
    "print(\"\\nPortfolio Diversification Insights:\")\n",
    "high_common = [\n",
    "    market for i, market in enumerate(df.columns) if abs(loadings_rotated[i, 0]) > 0.8\n",
    "]\n",
    "differentiated = [\n",
    "    market\n",
    "    for i, market in enumerate(df.columns)\n",
    "    if abs(loadings_rotated[i, 1]) > abs(loadings_rotated[i, 0])\n",
    "]\n",
    "\n",
    "if high_common:\n",
    "    print(f\"  Markets with high common factor exposure: {', '.join(high_common)}\")\n",
    "    print(\"  → These markets move together; limited diversification benefit\")\n",
    "\n",
    "if differentiated:\n",
    "    print(f\"  Markets with differentiated patterns: {', '.join(differentiated)}\")\n",
    "    print(\"  → These markets may provide diversification opportunities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b17c7ad",
   "metadata": {},
   "source": [
    "## Model Validation and Goodness of Fit\n",
    "\n",
    "Assess how well our 2-factor model reproduces the observed correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dac88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"MODEL VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate observed correlation matrix\n",
    "observed_corr = np.corrcoef(X_scaled.T)\n",
    "\n",
    "# Calculate reproduced correlation matrix from factor model\n",
    "# R̂ = ΛΛ' + Ψ (where Ψ is diagonal uniquenesses matrix)\n",
    "reproduced_corr = loadings_rotated @ loadings_rotated.T + np.diag(uniquenesses)\n",
    "\n",
    "# Calculate residual correlation matrix\n",
    "residual_corr = observed_corr - reproduced_corr\n",
    "\n",
    "# Model fit statistics\n",
    "total_corr_sum_sq = np.sum(observed_corr**2)\n",
    "residual_sum_sq = np.sum(residual_corr**2)\n",
    "fit_index = 1 - (residual_sum_sq / total_corr_sum_sq)\n",
    "\n",
    "print(\"Model Fit Assessment:\")\n",
    "print(f\"  Correlation fit index: {fit_index:.3f}\")\n",
    "print(f\"  Interpretation: {fit_index:.1%} of correlations explained by factor model\")\n",
    "\n",
    "# Root mean squared residual\n",
    "rmsr = np.sqrt(np.mean(residual_corr[np.triu_indices_from(residual_corr, k=1)] ** 2))\n",
    "print(f\"  Root Mean Square Residual (RMSR): {rmsr:.4f}\")\n",
    "print(\n",
    "    f\"  Interpretation: {'Good fit' if rmsr < 0.05 else 'Acceptable fit' if rmsr < 0.10 else 'Poor fit'}\"\n",
    ")\n",
    "\n",
    "# Show largest residuals\n",
    "residual_triu = residual_corr[np.triu_indices_from(residual_corr, k=1)]\n",
    "large_residuals = np.abs(residual_triu) > 0.1\n",
    "\n",
    "if np.any(large_residuals):\n",
    "    print(\"\\n  Large residual correlations (>0.1) detected:\")\n",
    "    triu_indices = list(zip(*np.triu_indices_from(residual_corr, k=1), strict=False))\n",
    "    for i, is_large in enumerate(large_residuals):\n",
    "        if is_large:\n",
    "            row, col = triu_indices[i]\n",
    "            market1, market2 = df.columns[row], df.columns[col]\n",
    "            print(f\"    {market1}-{market2}: {residual_triu[i]:.3f}\")\n",
    "    print(\"  → Consider additional factors or model modifications\")\n",
    "else:\n",
    "    print(\"  ✓ All residual correlations < 0.1 - Good model fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8830e96",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "This Factor Analysis of European stock markets reveals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SUMMARY AND CONCLUSIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Factor Structure Identified:\")\n",
    "print(\n",
    "    f\"  • {n_factors} common factors explain {proportion_common_variance:.1%} of market covariance\"\n",
    ")\n",
    "print(\"  • Factor 1: Common European market factor (systematic risk)\")\n",
    "print(\"  • Factor 2: Regional differentiation factor (idiosyncratic patterns)\")\n",
    "\n",
    "print(\"\\nKey Financial Insights:\")\n",
    "print(f\"  • Average systematic risk: {systematic_risk:.1%}\")\n",
    "print(f\"  • Average idiosyncratic risk: {idiosyncratic_risk:.1%}\")\n",
    "print(f\"  • Model explains {fit_index:.1%} of observed correlations\")\n",
    "\n",
    "print(\"\\nMarket Integration:\")\n",
    "most_integrated = df.columns[np.argmax(communalities)]\n",
    "least_integrated = df.columns[np.argmin(communalities)]\n",
    "print(\n",
    "    f\"  • Most integrated market: {most_integrated} (h² = {np.max(communalities):.3f})\"\n",
    ")\n",
    "print(\n",
    "    f\"  • Least integrated market: {least_integrated} (h² = {np.min(communalities):.3f})\"\n",
    ")\n",
    "\n",
    "print(\"\\nPractical Applications:\")\n",
    "print(\"  • Portfolio risk management: Identify common risk factors\")\n",
    "print(\"  • Diversification strategy: Focus on markets with low communalities\")\n",
    "print(\"  • Risk modeling: Use factor loadings for multi-factor risk models\")\n",
    "print(\"  • Market timing: Monitor common factor vs idiosyncratic movements\")\n",
    "\n",
    "logger.info(\"European Stock Markets Factor Analysis completed successfully\")\n",
    "print(f\"\\nFactor analysis completed. Results saved to: {script_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1088fd5",
   "metadata": {},
   "source": [
    "## Next Steps for Advanced Applications\n",
    "\n",
    "This analysis provides foundation for:\n",
    "- **Dynamic Factor Analysis**: Time-varying factor loadings\n",
    "- **Regime-Switching Models**: Different factors in different market states\n",
    "- **Higher-Frequency Analysis**: Intraday factor structures\n",
    "- **Cross-Asset Applications**: Bonds, currencies, commodities factor analysis\n",
    "- **Risk Attribution**: Performance attribution to systematic vs idiosyncratic sources"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
